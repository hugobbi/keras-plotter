{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJdPAuFqKDVc"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_wNKXU3BGuWq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-22 10:59:09.346224: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-02-22 10:59:09.374636: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-02-22 10:59:09.490484: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-22 10:59:09.490603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-22 10:59:09.505106: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-22 10:59:09.535610: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-02-22 10:59:09.536123: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-22 10:59:10.624078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import numpy as np\n",
        "from typing import Tuple, List\n",
        "from attr import define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow **IS NOT** using the GPU\n"
          ]
        }
      ],
      "source": [
        "if tf.config.list_physical_devices('GPU'):\n",
        "  print(\"TensorFlow **IS** using the GPU\")\n",
        "else:\n",
        "  print(\"TensorFlow **IS NOT** using the GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LngGn84xMeV6"
      },
      "source": [
        "Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S9iFws82Mdmi"
      },
      "outputs": [],
      "source": [
        "def normalize_ndarray(array: np.ndarray) -> np.ndarray:\n",
        "\n",
        "    max = np.max(array)\n",
        "    min = np.min(array)\n",
        "\n",
        "    if (max - min) == 0:\n",
        "        return array\n",
        "    else:\n",
        "        return (array - min) / (max - min)\n",
        "\n",
        "\n",
        "def show_dataset(\n",
        "    x_data_left: np.array, x_data_right: np.array, y_data: np.array, num_images: int\n",
        ") -> None:\n",
        "    fig1, ax = plt.subplots(num_images, 2, figsize=(2.4, num_images * 1.2))\n",
        "    for i in range(num_images):\n",
        "        ax[i, 0].imshow(x_data_left[i], cmap=\"binary\", vmax=1)\n",
        "        ax[i, 1].imshow(x_data_right[i], cmap=\"binary\", vmax=1)\n",
        "\n",
        "        ax[i, 1].text(45, 18, f\"{y_data[i]}\", fontsize=20)\n",
        "\n",
        "        ax[i, 0].set_xticks([])\n",
        "        ax[i, 0].set_yticks([])\n",
        "        ax[i, 1].set_xticks([])\n",
        "        ax[i, 1].set_yticks([])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_single_digit(x_data, y_data, index, print_index: bool):\n",
        "    image = x_data[index]\n",
        "    digit = y_data[index]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(3, 3))\n",
        "\n",
        "    ax1.imshow(image, cmap=\"binary\", vmax=1)\n",
        "    ax1.set_title(\"Image\")\n",
        "    ax1.set_aspect(\"equal\")\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "\n",
        "    if not print_index:\n",
        "        ax2.set_title(\"Digit\")\n",
        "        ax2.text(0.4, 0.5, f\"{digit}\", fontsize=20)\n",
        "    else:\n",
        "        ax2.set_title(\"Index\")\n",
        "        ax2.text(0.4, 0.5, f\"{index}\", fontsize=20)\n",
        "\n",
        "    ax2.set_aspect(\"equal\")\n",
        "    ax2.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_double_digit(x_data_left, x_data_right, y_data, index):\n",
        "    image_left = x_data_left[index]\n",
        "    image_right = x_data_right[index]\n",
        "    digit = y_data[index]\n",
        "\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(3, 3))\n",
        "\n",
        "    ax1.imshow(image_left, cmap=\"binary\", vmax=1)\n",
        "    ax1.set_title(\"Image\")\n",
        "    ax1.set_aspect(\"equal\")\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "\n",
        "    ax2.imshow(image_right, cmap=\"binary\", vmax=1)\n",
        "    ax2.set_title(\"Image\")\n",
        "    ax2.set_aspect(\"equal\")\n",
        "    ax2.set_xticks([])\n",
        "    ax2.set_yticks([])\n",
        "\n",
        "    ax3.set_title(\"Digit\")\n",
        "    ax3.text(0.4, 0.5, f\"{digit}\", fontsize=20)\n",
        "    ax3.set_aspect(\"equal\")\n",
        "    ax3.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdTuPFiVReAK"
      },
      "source": [
        "Generating dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Gowf75R8Rf6L"
      },
      "outputs": [],
      "source": [
        "def shuffle_visual_field_dataset(\n",
        "    x_data: np.array, y_data: np.array\n",
        ") -> Tuple[np.array, np.array]:\n",
        "    # shuffles dataset comprised of two arrays\n",
        "\n",
        "    n = len(y_data)\n",
        "    unique_indices = np.random.permutation(\n",
        "        n\n",
        "    )  # generates random permutation of list in range(0, n)\n",
        "    x_data = x_data[unique_indices]\n",
        "    y_data = y_data[unique_indices]\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "\n",
        "def shuffle_two_visual_fields_dataset(\n",
        "    x_data_left: np.array, x_data_right: np.array, y_data: np.array\n",
        ") -> Tuple[np.array, np.array, np.array]:\n",
        "    # Shuffles dataset comprised of three arrays\n",
        "\n",
        "    n = len(y_data)\n",
        "    unique_indices = np.random.permutation(\n",
        "        n\n",
        "    )  # generates random permutation of list in range(0, n)\n",
        "    x_data_left = x_data_left[unique_indices]\n",
        "    x_data_right = x_data_right[unique_indices]\n",
        "    y_data = y_data[unique_indices]\n",
        "\n",
        "    return x_data_left, x_data_right, y_data\n",
        "\n",
        "\n",
        "def shuffle_and_double_dataset(\n",
        "    x_data: np.array, y_data: np.array\n",
        ") -> Tuple[np.array, np.array]:\n",
        "    x_data1, y_data1 = shuffle_visual_field_dataset(x_data, y_data)\n",
        "    x_data2, y_data2 = shuffle_visual_field_dataset(x_data, y_data)\n",
        "\n",
        "    x_data_concatenated = np.concatenate((x_data1, x_data2))\n",
        "    y_data_concatenated = np.concatenate((y_data1, y_data2))\n",
        "\n",
        "    return x_data_concatenated, y_data_concatenated\n",
        "\n",
        "\n",
        "def build_visual_field_data(\n",
        "    x_data: np.array, y_data: np.array, n: float\n",
        ") -> Tuple[np.array, np.array]:\n",
        "    \"\"\"\n",
        "    Builds the dataset for a single visual field, choosing random values from the input.\n",
        "\n",
        "    Input:\n",
        "    x_data: np.array(np.ndarray): array of two-dimensional arrays corresponding to the pixel values of digits of the MNIST dataset.\n",
        "    y_data: np.array(int): corresponding value of the digit represented by x_data.\n",
        "    n: float: size of the final dataset\n",
        "\n",
        "    Output:\n",
        "    x_data_right: np.array(np.ndarray): array of two-dimensional arrays corresponding to the pixel values of digits of the MNIST dataset.\n",
        "    y_data: np.array(int): corresponding value of the digit of the visual field.\n",
        "    \"\"\"\n",
        "\n",
        "    original_size = len(y_data)\n",
        "    random_indices = np.random.choice(np.arange(0, original_size), n)\n",
        "    x_data_visual_field = x_data[random_indices]\n",
        "    y_data_visual_field = y_data[random_indices]\n",
        "\n",
        "    return x_data_visual_field, y_data_visual_field\n",
        "\n",
        "\n",
        "def build_double_visual_fields_dataset(\n",
        "    x_data: np.array,\n",
        "    y_data: np.array,\n",
        "    final_size: float = 4,\n",
        "    proportion_cs: float = 0.5,\n",
        "    proportion_left: float = 0.5,\n",
        "    full_attention_value: float = 1,\n",
        "    reduced_attention_value: float = 0.5,\n",
        "    ss_attention_value: float = 0.5,\n",
        ") -> Tuple[np.array, np.array, np.array]:\n",
        "    \"\"\"\n",
        "    Builds an entire double visual fields dataset, comprised of two visual fields, left and right, and an array of the corresponding answer value for both visual fields.\n",
        "\n",
        "    Input:\n",
        "    x_data: np.array(np.ndarray): array of two-dimensional arrays corresponding to the pixel values of digits of the MNIST dataset.\n",
        "    y_data: np.array(int): corresponding value of the digit represented by x_data.\n",
        "    final_size: float: how many times the final dataset is bigger than the input data. Default is 4.\n",
        "    proportion_cs: float: proportion of entries in the final dataset that have CS over SS. Default is 0.5.\n",
        "    proportion_left: float: proportion of entries in the final dataset that have attention on the left visual field. Default is 0.5.\n",
        "    full_attention_value: float: value of the full attention in CS. Default is 1.\n",
        "    reduced_attention_value: float: value of the reduced attention in CS. Default is 0.5.\n",
        "    ss_attention_value: float: value of the attention for SS. Default is 0.5.\n",
        "\n",
        "    Output:\n",
        "    x_data_left: np.array(np.ndarray): array of two-dimensional arrays corresponding to the pixel values of digits of the MNIST dataset with a determined attention.\n",
        "    x_data_right: np.array(np.ndarray): array of two-dimensional arrays corresponding to the pixel values of digits of the MNIST dataset with a determined attention.\n",
        "    y_data: np.array(int): corresponding value of the digit that has most attention considering both visual fields.\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(y_data) * final_size\n",
        "    x_data_left, y_data_left = build_visual_field_data(x_data, y_data, n)\n",
        "    x_data_right, y_data_right = build_visual_field_data(x_data, y_data, n)\n",
        "\n",
        "    y_data_final = np.zeros(n, dtype=int)\n",
        "    for i in range(n):\n",
        "        data_with_cs = np.random.choice(\n",
        "            [False, True], p=[1 - proportion_cs, proportion_cs]\n",
        "        )\n",
        "        data_with_left_attention = np.random.choice(\n",
        "            [False, True], p=[1 - proportion_left, proportion_left]\n",
        "        )\n",
        "\n",
        "        # determines value of attention if dataset entry is CS or SS\n",
        "        if data_with_cs:\n",
        "            attention = full_attention_value\n",
        "            no_attention = reduced_attention_value\n",
        "        else:\n",
        "            attention = ss_attention_value\n",
        "            no_attention = 0\n",
        "\n",
        "        # determines which visual field has attention\n",
        "        if data_with_left_attention:\n",
        "            x_data_left[i] *= attention\n",
        "            x_data_right[i] *= no_attention\n",
        "            y_data_final[i] = y_data_left[i]\n",
        "        else:\n",
        "            x_data_left[i] *= no_attention\n",
        "            x_data_right[i] *= attention\n",
        "            y_data_final[i] = y_data_right[i]\n",
        "\n",
        "    return x_data_left, x_data_right, y_data_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QcrFX-ZKPLJ"
      },
      "source": [
        "Importing dataset and normalizing input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NoeV7ONKIaO",
        "outputId": "a2415cb9-a248-4cd1-d308-a7eccbe5830b"
      },
      "outputs": [],
      "source": [
        "mnist_dataset = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist_dataset.load_data()\n",
        "x_train = normalize_ndarray(x_train)\n",
        "x_test = normalize_ndarray(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckbuOoWUXDhq"
      },
      "source": [
        "Building training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NOUxWDbKKCai"
      },
      "outputs": [],
      "source": [
        "x_train_left, x_train_right, y_train_final = build_double_visual_fields_dataset(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    final_size=4,\n",
        "    proportion_cs=0.5,\n",
        "    proportion_left=0.5,\n",
        "    full_attention_value=1,\n",
        "    reduced_attention_value=0.5,\n",
        "    ss_attention_value=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8gGTdb9X842"
      },
      "source": [
        "Showing training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "collapsed": true,
        "id": "1vxEzCZAYBSj",
        "outputId": "9bb42de4-e298-4486-8838-ab857e41f737"
      },
      "outputs": [],
      "source": [
        "show_dataset(x_train_left, x_train_right, y_train_final, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45nm6T8SeyZN"
      },
      "source": [
        "Building testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "r4oz4CJze01X"
      },
      "outputs": [],
      "source": [
        "x_test_left, x_test_right, y_test_final = build_double_visual_fields_dataset(\n",
        "    x_test,\n",
        "    y_test,\n",
        "    final_size=1,\n",
        "    proportion_cs=0.5,\n",
        "    proportion_left=0.5,\n",
        "    full_attention_value=1,\n",
        "    reduced_attention_value=0.5,\n",
        "    ss_attention_value=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-osk16rCfuX_"
      },
      "source": [
        "Showing testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "collapsed": true,
        "id": "ep873be-fxmQ",
        "outputId": "a0135e78-0332-46ee-d357-c94a3626e763"
      },
      "outputs": [],
      "source": [
        "show_dataset(x_test_left, x_test_right, y_test_final, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlPMetPM41tm"
      },
      "source": [
        "**Visualizing Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABhrN7amSA4-"
      },
      "source": [
        "Plotting Neural Network dataclasses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yP1vZCccSAEp"
      },
      "outputs": [],
      "source": [
        "@define\n",
        "class Position:\n",
        "    \"\"\"\n",
        "    Class representing a position with two coordinates\n",
        "    \"\"\"\n",
        "\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "    def copy(self):\n",
        "        return Position(self.x, self.y)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Position({self.x}, {self.y})\"\n",
        "\n",
        "\n",
        "@define\n",
        "class Neuron:\n",
        "    \"\"\"\n",
        "    Class representing a neuron in the Neural Network plot\n",
        "    \"\"\"\n",
        "\n",
        "    activation: float\n",
        "    position: Position\n",
        "    radius: float\n",
        "\n",
        "\n",
        "@define\n",
        "class Layer:\n",
        "    \"\"\"\n",
        "    Class representing a layer in the Neural Network plot\n",
        "    \"\"\"\n",
        "\n",
        "    model: keras.layers\n",
        "    activations: List[float] | None = None\n",
        "    position: Position | None = None\n",
        "    neurons: List[Neuron] | None = None\n",
        "    num_neurons: int | None = None\n",
        "\n",
        "    def __attrs_post_init__(self):\n",
        "        self.neurons = ([])  # This is needed to solve bug where list is not empty at start\n",
        "        self.num_neurons = self.model.output_shape[-1]\n",
        "\n",
        "    def is_output_layer(self):\n",
        "        return not bool(self.model._outbound_nodes)\n",
        "\n",
        "    def set_y_position(self, y_position):\n",
        "        self.position.y = y_position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa60X1SZPEDm"
      },
      "source": [
        "Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VF_-HBr9PK2f"
      },
      "outputs": [],
      "source": [
        "def get_index_of_digit(y_data, digit):\n",
        "    \"\"\"\n",
        "    Return index of first occurance of digit in the dataset, if digit is not found, returns -1\n",
        "    \"\"\"\n",
        "    n = len(y_data)\n",
        "    index = 0\n",
        "    while y_data[index] != digit and index < n - 1:\n",
        "        index += 1\n",
        "\n",
        "    return index if y_data[index] == digit else -1\n",
        "\n",
        "\n",
        "def get_single_visual_field_digit(x_data, y_data, digit):\n",
        "    \"\"\"\n",
        "    Returns the first instance of the digit in the Single Visual Field dataset\n",
        "    \"\"\"\n",
        "    index = get_index_of_digit(y_data, digit)\n",
        "\n",
        "    return np.array([x_data[index]])\n",
        "\n",
        "\n",
        "def get_double_visual_field_digit(x_data_left, x_data_right, y_data, digit):\n",
        "    \"\"\"\n",
        "    Returns the first instance of the digit in the Double Visual Field dataset\n",
        "    \"\"\"\n",
        "    index = get_index_of_digit(y_data, digit)\n",
        "    if index == -1:\n",
        "        print(\"Error! Digit wasn't found!\")\n",
        "\n",
        "    return np.array([x_data_left[index]]), np.array([x_data_right[index]])\n",
        "\n",
        "\n",
        "def get_activations_single_visual_field(model, input_digit, x_data, y_data):\n",
        "    \"\"\"\n",
        "    Generates the individual neuron activation values for a prediction with a single visual field Neural Network\n",
        "    \"\"\"\n",
        "\n",
        "    # Creating intermediate models\n",
        "    intermediate_layer_models = [\n",
        "        keras.models.Model(\n",
        "            inputs=model.input, outputs=layer.output\n",
        "        )\n",
        "        for layer in model.layers\n",
        "    ]\n",
        "\n",
        "    # Getting activations\n",
        "    input_data = get_single_visual_field_digit(x_train, y_train, input_digit)\n",
        "    activations = [\n",
        "        intermediate_layer_model(input_data)[0]\n",
        "        for intermediate_layer_model in intermediate_layer_models\n",
        "    ]  # Predict result is returned inside a list, hence we get the first element of that list\n",
        "\n",
        "    return activations\n",
        "\n",
        "def get_activations_double_visual_field(\n",
        "    model,\n",
        "    x_data=None,\n",
        "    left_vf_digit: Tuple[int, float] = None,\n",
        "    right_vf_digit: Tuple[int, float] = None,\n",
        "    x_data_left=None,\n",
        "    x_data_right=None,\n",
        "    input_digit=None,\n",
        "    y_data=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates the individual neuron activation values for a prediction with a double visual field Neural Network\n",
        "    \"\"\"\n",
        "\n",
        "    # Creating intermediate models\n",
        "    intermediate_layer_models = [\n",
        "        keras.models.Model(\n",
        "            inputs=model.input, outputs=layer.output\n",
        "        )\n",
        "        for layer in model.layers\n",
        "    ]\n",
        "\n",
        "    # Getting activations\n",
        "    if left_vf_digit is None:\n",
        "        input_data_left, input_data_right = get_double_visual_field_digit(\n",
        "            x_data_left, x_data_right, y_data, input_digit\n",
        "        )\n",
        "    else:\n",
        "        left_idx, left_attention = left_vf_digit\n",
        "        input_data_left = np.array([x_data[left_idx]]) * left_attention\n",
        "        right_idx, right_attention = right_vf_digit\n",
        "        input_data_right = np.array([x_data[right_idx]]) * right_attention\n",
        "    activations = [\n",
        "        intermediate_layer_model([input_data_left, input_data_right])[0]\n",
        "        for intermediate_layer_model in intermediate_layer_models\n",
        "    ]   # Model result is returned inside a list, hence we get the first element of that list\n",
        "\n",
        "    return activations\n",
        "\n",
        "\n",
        "def get_image(neural_activation):\n",
        "    \"\"\"\n",
        "    Transforms the activity of the neurons of a Neural Network into a 2D matrix that can be seen as an image with plt.imshow\n",
        "    \"\"\"\n",
        "    num_neurons = len(neural_activation)\n",
        "    num_rows = int(np.sqrt(num_neurons))\n",
        "    num_cols = int(np.ceil(num_neurons / num_rows))\n",
        "\n",
        "    return np.array(neural_activation).reshape(num_rows, num_cols)\n",
        "\n",
        "\n",
        "def generate_image_annotation_box(\n",
        "    image, position, size, border_size=1, border_color=\"black\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates the AB of the image to be positioned in the figure\n",
        "    \"\"\"\n",
        "    img = OffsetImage(image, zoom=size)\n",
        "    ab = AnnotationBbox(img, (position.x, position.y), frameon=False, pad=0)\n",
        "\n",
        "    return ab\n",
        "\n",
        "\n",
        "def generate_image_annotation_box_grayscale(\n",
        "    image, position, size, border_width=1, border_color=\"black\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates the AnnotationBbox of the grayscale image with a border to be positioned in the figure\n",
        "    \"\"\"\n",
        "\n",
        "    img = OffsetImage(\n",
        "        image, zoom=size, cmap=\"binary\", norm=plt.Normalize(vmin=0, vmax=1)\n",
        "    )\n",
        "    ab = AnnotationBbox(\n",
        "        img,\n",
        "        (position.x, position.y),\n",
        "        frameon=True,\n",
        "        pad=0,\n",
        "        bboxprops=dict(\n",
        "            edgecolor=border_color, linewidth=border_width, boxstyle=\"square,pad=0.1\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return ab\n",
        "\n",
        "\n",
        "def get_number_of_neurons_in_layer(layer):\n",
        "    \"\"\"\n",
        "    Returns the number of neurons present in a layer of a Neural Network\n",
        "    \"\"\"\n",
        "    return layer.output_shape[-1]\n",
        "\n",
        "\n",
        "def get_digit_from_y_spacing(total_position_plotted, y_spacing):\n",
        "    \"\"\"\n",
        "    Returns the digit an output neuron represents based on how many neurons have been plotted\n",
        "    \"\"\"\n",
        "    return np.ceil(total_position_plotted / y_spacing)\n",
        "\n",
        "\n",
        "def calculate_image_size(number_neurons):\n",
        "    \"\"\"\n",
        "    Computes size of image based on the number of neurons\n",
        "    \"\"\"\n",
        "    # Determined in tests\n",
        "    a = -5 / 1536\n",
        "    b = 533 / 96\n",
        "    return number_neurons * a + b\n",
        "\n",
        "\n",
        "def compute_sizes(top):\n",
        "    bottom = 1 - top\n",
        "    return top, bottom, bottom, top  # top, bottom, left, right\n",
        "\n",
        "\n",
        "def display_n_digits(x_data, y_data, digit: int, n: int):\n",
        "    size = len(y_data)\n",
        "    idx = 0\n",
        "    digits_found = 0\n",
        "    idx_last_digit = 0\n",
        "    while idx < size and digits_found < n:\n",
        "        if y_data[idx] == digit:\n",
        "            digits_found += 1\n",
        "            idx_last_digit = idx\n",
        "            show_single_digit(x_data, y_data, idx, print_index=True)\n",
        "        idx += 1\n",
        "\n",
        "def generate_output_models(model):\n",
        "  # Generate not trainable copy of model\n",
        "  model_copy = keras.models.clone_model(model)\n",
        "  for layer in model_copy.layers:\n",
        "    layer.trainable = False \n",
        "  \n",
        "  # Generate output models of each hidden layer\n",
        "  output_models = []\n",
        "  for layer in model_copy.layers[4:-2]:\n",
        "    out_model_input = model_copy.input # [left_input, right_input]\n",
        "    if 'left' in layer.name:\n",
        "        out_model_input = out_model_input[0]\n",
        "    elif 'right' in layer.name:\n",
        "        out_model_input = out_model_input[1]\n",
        "         \n",
        "    out_model_output = keras.layers.Dense(10, activation=\"softmax\")(layer.output)\n",
        "    out_model = keras.models.Model(\n",
        "      inputs=out_model_input, outputs=out_model_output\n",
        "    )\n",
        "    output_models.append(out_model)\n",
        "\n",
        "  return output_models\n",
        "\n",
        "def show_dataset_sizes(*data):\n",
        "    for d in data:\n",
        "        print(d.shape)\n",
        "\n",
        "def data_generator_double_visual_field(x_data_left, x_data_right, y_data, batch_size):\n",
        "    while True:\n",
        "        for i in range(0, len(y_data), batch_size):\n",
        "            yield [x_data_left[i:i+batch_size], x_data_right[i:i+batch_size]], y_data[i:i+batch_size]\n",
        "\n",
        "def data_generator_single_visual_field(x_data, y_data, batch_size):\n",
        "    while True:\n",
        "        for i in range(0, len(y_data), batch_size):\n",
        "            yield x_data[i:i+batch_size], y_data[i:i+batch_size]\n",
        "        \n",
        "def is_double_visual_field_model(model):\n",
        "    return type(model.input) == list # If input is a list, it is a double visual field model\n",
        "\n",
        "def is_left_visual_field_model(model):\n",
        "    return not is_double_visual_field_model(model) and 'left' in model.input.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KDF0o3MPTnl"
      },
      "source": [
        "Single visual field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "J2jFkGOJjfDk"
      },
      "outputs": [],
      "source": [
        "def display_single_visual_field_mnist_nn_execution(\n",
        "    model, x_data, y_data, input_digit, max_neurons, weight_plot_threshold\n",
        "):\n",
        "    \"\"\"\n",
        "    Receives a trained Neural Network model and and input digit, plotting the full neural network execution\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" Determining matplotlib figure parameters \"\"\"\n",
        "\n",
        "    fig = plt.figure(\n",
        "        figsize=(12, 12)\n",
        "    )  # TODO: generate size based on max number of neurons on layer\n",
        "    ax = fig.gca()\n",
        "    ax.axis(\"off\")\n",
        "    left, right, bottom, top = 0.1, 0.9, 0.1, 0.9\n",
        "\n",
        "    \"\"\" Calculating neural network activations \"\"\"\n",
        "\n",
        "    model_activations = get_activations_single_visual_field(\n",
        "        model, input_digit, x_data, y_data\n",
        "    )\n",
        "\n",
        "    \"\"\" Determining neural network figure parameters \"\"\"\n",
        "\n",
        "    model_number_of_layers = len(model.layers)\n",
        "    layer_spacing = (right - left) / (model_number_of_layers - 1)  # Space between each layer in the plot\n",
        "    connection_opacity = 0.2\n",
        "    color_connection_opacity = 0.5\n",
        "    image_y_position = 0.5\n",
        "    current_position = Position(left, image_y_position)  # Initial position\n",
        "\n",
        "    \"\"\" Plotting layers \"\"\"\n",
        "\n",
        "    plotted_layers = []\n",
        "    for i in range(model_number_of_layers):\n",
        "        model_layer = model.layers[i]\n",
        "        layer_activations = model_activations[i]  # TODO: Can raise error if number of activations not equal to number of layers\n",
        "\n",
        "        layer = Layer(model=model_layer, activations=layer_activations)\n",
        "        plotted_layers.append(layer)  # Adding layer to plotted layers\n",
        "\n",
        "        # Depending on the type of layer, different  figures will be created to represent it\n",
        "\n",
        "        # -- Flatten layer --\n",
        "        if \"flatten\" in layer.model.name:\n",
        "            # If it is the input layer\n",
        "            if i == 0:\n",
        "                layer.position = current_position.copy()\n",
        "                ab = generate_image_annotation_box_grayscale(\n",
        "                    get_image(layer.activations),\n",
        "                    layer.position,\n",
        "                    size=calculate_image_size(layer.num_neurons),\n",
        "                )\n",
        "                ax.add_artist(ab)\n",
        "\n",
        "        #  -- Dense layer --\n",
        "        elif \"dense\" in layer.model.name:\n",
        "\n",
        "            # -- Plotting neurons of dense layer --\n",
        "            if layer.num_neurons < max_neurons:  # Plotting individual neurons\n",
        "                current_position.x += layer_spacing\n",
        "                current_position.y = top\n",
        "                layer.position = current_position.copy()\n",
        "\n",
        "                previous_layer = plotted_layers[i - 1]  # Gets previous layer\n",
        "                layer_weights = normalize_ndarray(layer.model.get_weights()[0])\n",
        "\n",
        "                neuron_spacing = (top - bottom) / layer.num_neurons  # Space between each neuron in the plot\n",
        "                layer_activations = (\n",
        "                    normalize_ndarray(layer_activations)\n",
        "                    if not layer.is_output_layer()\n",
        "                    else layer_activations\n",
        "                )  # Normalize layer activations if it is not output layer\n",
        "                for idx, neuron_activation in enumerate(layer_activations):\n",
        "                    # -- Plotting each neuron --\n",
        "                    neuron = Neuron(\n",
        "                        neuron_activation,\n",
        "                        current_position.copy(),\n",
        "                        radius=neuron_spacing / 4,\n",
        "                    )\n",
        "                    layer.neurons.append(neuron)  # Adding neuron to plotted layer\n",
        "                    neuron_circle = plt.Circle(\n",
        "                        xy=(current_position.x, current_position.y),\n",
        "                        radius=neuron.radius,\n",
        "                        color=plt.cm.viridis(neuron_activation),\n",
        "                        ec=\"k\",\n",
        "                    )\n",
        "                    ax.add_artist(neuron_circle)  # Plots neuron\n",
        "\n",
        "                    # -- Plotting connections of neurons to previous layer --\n",
        "                    if (previous_layer.num_neurons < max_neurons):  # Checks if the number of neurons on the previous layer doesn't exceed the maximum to be plotted)\n",
        "                        layer_weights_neuron = layer_weights[:, idx]\n",
        "                        for previous_neuron, connection_weight in zip(previous_layer.neurons, layer_weights_neuron):\n",
        "                            # Checks if weight is above threshold\n",
        "                            if connection_weight >= weight_plot_threshold:\n",
        "                                connection = plt.Line2D(\n",
        "                                    [\n",
        "                                        current_position.x - neuron.radius,\n",
        "                                        previous_neuron.position.x + previous_neuron.radius,\n",
        "                                    ],\n",
        "                                    [current_position.y, previous_neuron.position.y],\n",
        "                                    color=plt.cm.viridis(connection_weight),\n",
        "                                    alpha=color_connection_opacity,\n",
        "                                )\n",
        "                                ax.add_artist(connection)  # Plots connection to previous neurons\n",
        "\n",
        "                    else:  # In case it does, only a single conncetion from each neuron will be shown connecting to the previous layer\n",
        "                        previous_layer_position = Position(previous_layer.position.x, previous_layer.position.y)  # Saves position of previous layer\n",
        "                        image_offset = 0.065\n",
        "                        connection = plt.Line2D(\n",
        "                            [\n",
        "                                current_position.x - neuron.radius,\n",
        "                                previous_layer_position.x + image_offset\n",
        "                            ],\n",
        "                            [current_position.y, previous_layer_position.y],\n",
        "                            color=\"k\",\n",
        "                            alpha=connection_opacity,\n",
        "                        )\n",
        "                        ax.add_artist(connection)  # Plots connection to image\n",
        "\n",
        "                    # -- Plotting digit and activation if it is the output layer --\n",
        "                    if i == model_number_of_layers - 1:\n",
        "                        neuron_digit = get_digit_from_y_spacing(\n",
        "                            (top - current_position.y), neuron_spacing\n",
        "                        )\n",
        "                        text_offset_x = 0.001\n",
        "                        text_offset_y = 0.005\n",
        "                        text = plt.Text(\n",
        "                            current_position.x + neuron.radius + text_offset_x,\n",
        "                            current_position.y - text_offset_y,\n",
        "                            f\"{neuron_digit}: {neuron_activation:.4f}\",\n",
        "                            fontsize=8,\n",
        "                            color=\"k\",\n",
        "                        )\n",
        "                        ax.add_artist(text)  # Adds corresponding digit and activation of neuron\n",
        "\n",
        "                    # Changes the current y position to plot the next neuron\n",
        "                    current_position.y -= neuron_spacing\n",
        "\n",
        "            else:  # If the number of neurons in the layer exceeds the maximum TODO: Color connections if previous layer has few neurons\n",
        "                current_position.x += layer_spacing\n",
        "                current_position.y = image_y_position\n",
        "                layer.position = current_position.copy()\n",
        "\n",
        "                # -- Plotting layer as image --\n",
        "                ab = generate_image_annotation_box(\n",
        "                    get_image(layer.activations),\n",
        "                    layer.position,\n",
        "                    size=calculate_image_size(layer.num_neurons),\n",
        "                )\n",
        "                ax.add_artist(ab)\n",
        "\n",
        "                # -- Plotting connections of image to previous layer --\n",
        "                previous_layer = plotted_layers[i - 1]  # Gets previous layer\n",
        "                if (previous_layer.num_neurons < max_neurons):  # Checks if the number of neurons on the previous layer doesn't exceed the maximum to be plotted\n",
        "                    for previous_neuron in previous_layer.neurons:\n",
        "                        connection = plt.Line2D(\n",
        "                            [\n",
        "                                current_position.x,\n",
        "                                previous_neuron.position.x + previous_neuron.radius,\n",
        "                            ],\n",
        "                            [current_position.y, previous_neuron.position.y],\n",
        "                            color=\"k\",\n",
        "                            alpha=connection_opacity,\n",
        "                        )\n",
        "                        ax.add_artist(connection)  # Plots connection to previous neurons\n",
        "                else:  # If previous layer exceeds the maximum number of neurons plotted\n",
        "                    connection = plt.Line2D(\n",
        "                        [current_position.x, previous_layer.position.x],\n",
        "                        [current_position.y, previous_layer.position.y],\n",
        "                        color=\"k\",\n",
        "                        alpha=connection_opacity,\n",
        "                    )\n",
        "                    ax.add_artist(connection)  # Plots connection to previous neurons\n",
        "\n",
        "    fig.savefig(\"nn.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Double visual field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Pwa-_7o9X9XH"
      },
      "outputs": [],
      "source": [
        "def display_double_visual_field_mnist_nn_execution(\n",
        "    model,\n",
        "    max_neurons,\n",
        "    weight_plot_threshold,\n",
        "    x_data=None,\n",
        "    left_vf_digit: Tuple[int, float] = None,\n",
        "    right_vf_digit: Tuple[int, float] = None,\n",
        "    x_data_left=None,\n",
        "    x_data_right=None,\n",
        "    y_data=None,\n",
        "    input_digit=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Receives a trained Neural Network model with two visual fields and and input digit, plotting the full neural network execution.\n",
        "    Obseravtion: this code currently doesn't support Convolution or MaxPool layers and layers with too many neurons.\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\" Determining matplotlib figure parameters \"\"\"\n",
        "\n",
        "    fig = plt.figure(\n",
        "        figsize=(24, 24)\n",
        "    )  # TODO: generate size based on max number of neurons on layer\n",
        "    ax = fig.gca()\n",
        "    ax.axis(\"off\")\n",
        "    top, bottom, left, right = compute_sizes(top=0.98)\n",
        "    precision = 4  # Number of digits of precision when calculating positions\n",
        "    middle = round((top + bottom) / 2, ndigits=precision)\n",
        "    middle_spacing = 0.02\n",
        "\n",
        "    \"\"\" Calculating neural network activations \"\"\"\n",
        "\n",
        "    if left_vf_digit is None:\n",
        "        model_activations = get_activations_double_visual_field(\n",
        "            model, input_digit, x_data_left, x_data_right, y_data\n",
        "        )\n",
        "    else:\n",
        "        model_activations = get_activations_double_visual_field(\n",
        "            model, x_data, left_vf_digit, right_vf_digit\n",
        "        )\n",
        "\n",
        "    \"\"\" Determining neural network figure parameters \"\"\"\n",
        "\n",
        "    model_number_of_layers = len(model.layers)\n",
        "    layer_spacing = (right - left) / (model_number_of_layers - 1)  # Space between each layer in the plot\n",
        "    connection_opacity = 0.2\n",
        "    color_connection_opacity = 0.5\n",
        "    left_vf_position = Position(left, 1.5 * middle)  # Initial position of left visual field\n",
        "    right_vf_position = Position(left, 0.5 * middle)  # Initial position of right visual field\n",
        "\n",
        "    \"\"\" Plotting layers \"\"\"\n",
        "\n",
        "    plotted_layers_left_vf = []\n",
        "    plotted_layers_right_vf = []\n",
        "    plotted_layers_concatenated = []\n",
        "    layers_reference = None\n",
        "\n",
        "    # Input layers\n",
        "\n",
        "    # Left\n",
        "    input_layer_left = Layer(model=model.layers[2], activations=model_activations[2])\n",
        "    input_layer_left.position = left_vf_position.copy()\n",
        "    ab = generate_image_annotation_box_grayscale(\n",
        "        get_image(input_layer_left.activations),\n",
        "        input_layer_left.position,\n",
        "        size=calculate_image_size(input_layer_left.num_neurons),\n",
        "    )\n",
        "    ax.add_artist(ab)\n",
        "    plotted_layers_left_vf.append(input_layer_left)\n",
        "\n",
        "    # Right\n",
        "    input_layer_right = Layer(model=model.layers[3], activations=model_activations[3])\n",
        "    input_layer_right.position = right_vf_position.copy()\n",
        "    ab = generate_image_annotation_box_grayscale(\n",
        "        get_image(input_layer_right.activations),\n",
        "        input_layer_right.position,\n",
        "        size=calculate_image_size(input_layer_right.num_neurons),\n",
        "    )\n",
        "    ax.add_artist(ab)\n",
        "    plotted_layers_right_vf.append(input_layer_right)\n",
        "\n",
        "    # Plotting the rest of the layers\n",
        "\n",
        "    # Variables used to keep track of each visual field\n",
        "    left_idx = 0\n",
        "    right_idx = 0\n",
        "    concat_idx = 0\n",
        "    concat_left_idx = 0\n",
        "    concat_right_idx = 0\n",
        "    neuron_digit = 0\n",
        "    is_concatenated = False\n",
        "    is_concatenate_layer = False\n",
        "    is_output_layer = False\n",
        "    plot_connections_left_vf = True  # Used in concatenate layer\n",
        "\n",
        "    # Plotting layers, either left vf, right vf or concatenated vf\n",
        "    for i in range(4, model_number_of_layers):\n",
        "        model_layer = model.layers[i]\n",
        "        layer_activations = model_activations[i]\n",
        "        layer = Layer(model=model_layer, activations=layer_activations)\n",
        "\n",
        "        # Determining if it is plotting left vf, right vf or concatenated vf\n",
        "        vf_top = top\n",
        "        vf_bottom = bottom\n",
        "        if \"left\" in layer.model.name:\n",
        "            layers_reference = plotted_layers_left_vf\n",
        "            left_idx += 1\n",
        "            layers_idx = left_idx\n",
        "            vf_bottom = middle + middle_spacing\n",
        "        elif \"right\" in layer.model.name:\n",
        "            layers_reference = plotted_layers_right_vf\n",
        "            right_idx += 1\n",
        "            layers_idx = right_idx\n",
        "            vf_top = middle - middle_spacing\n",
        "        elif \"concatenate\" in layer.model.name:\n",
        "            is_concatenate_layer = True\n",
        "            is_concatenated = True\n",
        "            layers_reference = plotted_layers_concatenated\n",
        "            layers_idx = concat_idx\n",
        "        elif is_concatenated:\n",
        "            concat_idx += 1\n",
        "            layers_idx = concat_idx\n",
        "        if i == model_number_of_layers - 1:\n",
        "            is_output_layer = True\n",
        "            output_offset = 0.2\n",
        "            vf_top = middle + output_offset\n",
        "            vf_bottom = middle - output_offset\n",
        "\n",
        "        # Adds current layer to plotted layers for each visual field\n",
        "        layers_reference.append(layer)\n",
        "\n",
        "        # Gets previous layer and layer weights\n",
        "        if not is_concatenate_layer:\n",
        "            layer_weights = normalize_ndarray(layer.model.get_weights()[0])  # Retruns 2d-array (num_neurons_previous_layer x num_neurons_current_layer)\n",
        "            # get_weights returns (weights, bias)\n",
        "            previous_layer = layers_reference[layers_idx - 1]\n",
        "            current_position = previous_layer.position.copy()\n",
        "\n",
        "        # Determines and saves position of current layer\n",
        "        current_position.x += layer_spacing\n",
        "        current_position.y = vf_top\n",
        "        layer.position = current_position.copy()\n",
        "\n",
        "        # Determines neuron spacing\n",
        "        neuron_spacing = (vf_top - vf_bottom) / layer.num_neurons\n",
        "        # ns_min = 0.05\n",
        "        # neuron_spacing = min(neuron_spacing, ns_min)\n",
        "\n",
        "        # Normalize layer activations if it is not output layer\n",
        "        layer_activations = (\n",
        "            normalize_ndarray(layer_activations)\n",
        "            if not is_output_layer\n",
        "            else layer_activations\n",
        "        )\n",
        "        for idx, neuron_activation in enumerate(layer_activations):\n",
        "            # -- Plotting each neuron --\n",
        "            neuron = Neuron(\n",
        "                neuron_activation, current_position.copy(), radius=neuron_spacing / 4\n",
        "            )\n",
        "            layer.neurons.append(neuron)  # Adding neuron to plotted layer\n",
        "            neuron_circle = plt.Circle(\n",
        "                xy=(current_position.x, current_position.y),\n",
        "                radius=neuron.radius,\n",
        "                color=plt.cm.viridis(neuron_activation),\n",
        "                ec=\"k\",\n",
        "            )\n",
        "            ax.add_artist(neuron_circle)  # Plots neuron\n",
        "\n",
        "            if not is_concatenate_layer:\n",
        "                # -- Plotting connections of neurons to previous layer --\n",
        "                if (previous_layer.num_neurons < max_neurons):  # Checks number of neurons on previous layer to decide if every connection will be plotted\n",
        "                    layer_weights_neuron = layer_weights[:, idx]\n",
        "                    for previous_neuron, connection_weight in zip(previous_layer.neurons, layer_weights_neuron):\n",
        "                        # Checks if weight is above threshold\n",
        "                        if connection_weight >= weight_plot_threshold:\n",
        "                            connection = plt.Line2D(\n",
        "                                [\n",
        "                                    current_position.x - neuron.radius,\n",
        "                                    previous_neuron.position.x + previous_neuron.radius,\n",
        "                                ],\n",
        "                                [current_position.y, previous_neuron.position.y],\n",
        "                                color=plt.cm.viridis(connection_weight),\n",
        "                                alpha=color_connection_opacity,\n",
        "                            )\n",
        "                            ax.add_artist(connection)  # Plots connection to previous neurons\n",
        "\n",
        "                else:  # In case it does, only a single conncetion from each neuron will be shown connecting to the previous layer\n",
        "                    image_offset = 0.032\n",
        "                    connection = plt.Line2D(\n",
        "                        [\n",
        "                            current_position.x - neuron.radius,\n",
        "                            previous_layer.position.x + image_offset,\n",
        "                        ],\n",
        "                        [current_position.y, previous_layer.position.y],\n",
        "                        color=\"k\",\n",
        "                        alpha=connection_opacity,\n",
        "                    )\n",
        "                    ax.add_artist(connection)  # Plots connection to image\n",
        "            # If it is the concatenate layer, show connection to corresponding neuron\n",
        "            else:\n",
        "                # Left\n",
        "                left_vf_layer = plotted_layers_left_vf[left_idx]\n",
        "                if concat_left_idx < left_vf_layer.num_neurons:\n",
        "                    previous_neuron = left_vf_layer.neurons[concat_left_idx]\n",
        "                    connection = plt.Line2D(\n",
        "                        [\n",
        "                            current_position.x - neuron.radius,\n",
        "                            previous_neuron.position.x + previous_neuron.radius,\n",
        "                        ],\n",
        "                        [current_position.y, previous_neuron.position.y],\n",
        "                        color=\"k\",\n",
        "                        alpha=connection_opacity,\n",
        "                    )\n",
        "                    ax.add_artist(connection)  # Plots connection to image\n",
        "                    concat_left_idx += 1\n",
        "\n",
        "                # Right\n",
        "                right_vf_layer = plotted_layers_right_vf[right_idx]\n",
        "                if (concat_right_idx < right_vf_layer.num_neurons and not plot_connections_left_vf):\n",
        "                    previous_neuron = right_vf_layer.neurons[concat_right_idx]\n",
        "                    connection = plt.Line2D(\n",
        "                        [\n",
        "                            current_position.x - neuron.radius,\n",
        "                            previous_neuron.position.x + previous_neuron.radius,\n",
        "                        ],\n",
        "                        [current_position.y, previous_neuron.position.y],\n",
        "                        color=\"k\",\n",
        "                        alpha=connection_opacity,\n",
        "                    )\n",
        "                    ax.add_artist(connection)  # Plots connection to image\n",
        "                    concat_right_idx += 1\n",
        "\n",
        "                plot_connections_left_vf = (\n",
        "                    False if concat_left_idx == left_vf_layer.num_neurons else True\n",
        "                )\n",
        "\n",
        "            # -- Plotting digit and activation if it is the output layer --\n",
        "            if is_output_layer:\n",
        "                text_offset_x = 0.001\n",
        "                text_offset_y = 0.005\n",
        "                text = plt.Text(\n",
        "                    current_position.x + neuron.radius + text_offset_x,\n",
        "                    current_position.y - text_offset_y,\n",
        "                    f\"{neuron_digit}: {neuron_activation:.4f}\",\n",
        "                    fontsize=8,\n",
        "                    color=\"k\",\n",
        "                )\n",
        "                ax.add_artist(text)  # Adds corresponding digit and activation of neuron to scene\n",
        "                neuron_digit += 1\n",
        "\n",
        "            # Changes the current y position to plot the next neuron\n",
        "            current_position.y -= neuron_spacing\n",
        "\n",
        "        if is_concatenate_layer:\n",
        "            is_concatenate_layer = False  # It will not be the concatenate layer anymore\n",
        "\n",
        "    fig.savefig(\"images/nn_dvf.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model_t1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5x1u0UJWqzm",
        "outputId": "123ada85-6081-42a3-e18c-83a048c6d639"
      },
      "outputs": [],
      "source": [
        "input_left = keras.layers.Input(shape=[28,28], name=\"input_left\")\n",
        "flatten_input_left  = keras.layers.Flatten()(input_left)\n",
        "\n",
        "input_right = keras.layers.Input(shape=[28,28], name=\"input_right\")\n",
        "flatten_input_right  = keras.layers.Flatten()(input_right)\n",
        "\n",
        "hidden_layer_left1 = keras.layers.Dense(30, activation=\"relu\", name=\"dense_left1\")(flatten_input_left)\n",
        "hidden_layer_left2 = keras.layers.Dense(20, activation=\"relu\", name=\"dense_left2\")(hidden_layer_left1)\n",
        "\n",
        "hidden_layer_right1 = keras.layers.Dense(30, activation=\"relu\", name=\"dense_right1\")(flatten_input_right)\n",
        "hidden_layer_right2 = keras.layers.Dense(20, activation=\"relu\", name=\"dense_right2\")(hidden_layer_right1)\n",
        "\n",
        "concatenate_layer = keras.layers.concatenate([hidden_layer_left2, hidden_layer_right2], name=\"concatenate\")\n",
        "\n",
        "hidden_layer_concat1 = keras.layers.Dense(40, activation=\"relu\", name=\"dense_concat1\")(concatenate_layer)\n",
        "hidden_layer_concat2 = keras.layers.Dense(30, activation=\"relu\", name=\"dense_concat2\")(hidden_layer_concat1)\n",
        "\n",
        "output_layer = keras.layers.Dense(10, activation=\"softmax\")(hidden_layer_concat2)\n",
        "\n",
        "model_t1 = keras.Model(inputs=[input_left, input_right], outputs=[output_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4342 - accuracy: 0.8728 - val_loss: 0.2126 - val_accuracy: 0.9363\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1831 - accuracy: 0.9468 - val_loss: 0.1639 - val_accuracy: 0.9491\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1351 - accuracy: 0.9605 - val_loss: 0.1455 - val_accuracy: 0.9554\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1102 - accuracy: 0.9674 - val_loss: 0.1446 - val_accuracy: 0.9571\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0940 - accuracy: 0.9723 - val_loss: 0.1435 - val_accuracy: 0.9587\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0815 - accuracy: 0.9760 - val_loss: 0.1471 - val_accuracy: 0.9583\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0722 - accuracy: 0.9786 - val_loss: 0.1527 - val_accuracy: 0.9602\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0643 - accuracy: 0.9811 - val_loss: 0.1481 - val_accuracy: 0.9612\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0581 - accuracy: 0.9826 - val_loss: 0.1514 - val_accuracy: 0.9614\n",
            "Epoch 10/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0527 - accuracy: 0.9842 - val_loss: 0.1565 - val_accuracy: 0.9619\n",
            "Epoch 11/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0474 - accuracy: 0.9857 - val_loss: 0.1629 - val_accuracy: 0.9618\n",
            "Epoch 12/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0441 - accuracy: 0.9862 - val_loss: 0.1715 - val_accuracy: 0.9622\n",
            "Epoch 13/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0411 - accuracy: 0.9872 - val_loss: 0.1730 - val_accuracy: 0.9632\n",
            "Epoch 14/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0386 - accuracy: 0.9881 - val_loss: 0.1843 - val_accuracy: 0.9615\n",
            "Epoch 15/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0359 - accuracy: 0.9887 - val_loss: 0.1839 - val_accuracy: 0.9639\n",
            "Epoch 16/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0343 - accuracy: 0.9891 - val_loss: 0.1794 - val_accuracy: 0.9649\n",
            "Epoch 17/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0317 - accuracy: 0.9900 - val_loss: 0.1931 - val_accuracy: 0.9619\n",
            "Epoch 18/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0308 - accuracy: 0.9902 - val_loss: 0.2021 - val_accuracy: 0.9632\n",
            "Epoch 19/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0276 - accuracy: 0.9912 - val_loss: 0.1840 - val_accuracy: 0.9636\n",
            "Epoch 20/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0281 - accuracy: 0.9911 - val_loss: 0.1962 - val_accuracy: 0.9644\n",
            "Epoch 21/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0267 - accuracy: 0.9914 - val_loss: 0.2034 - val_accuracy: 0.9628\n",
            "Epoch 22/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0261 - accuracy: 0.9917 - val_loss: 0.2134 - val_accuracy: 0.9627\n",
            "Epoch 23/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 0.2140 - val_accuracy: 0.9630\n",
            "Epoch 24/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0226 - accuracy: 0.9925 - val_loss: 0.2448 - val_accuracy: 0.9594\n",
            "Epoch 25/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0239 - accuracy: 0.9924 - val_loss: 0.2138 - val_accuracy: 0.9635\n",
            "Epoch 26/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 0.2346 - val_accuracy: 0.9601\n",
            "Epoch 27/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0223 - accuracy: 0.9929 - val_loss: 0.2229 - val_accuracy: 0.9649\n",
            "Epoch 28/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0202 - accuracy: 0.9935 - val_loss: 0.2240 - val_accuracy: 0.9630\n",
            "Epoch 29/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0199 - accuracy: 0.9935 - val_loss: 0.2411 - val_accuracy: 0.9614\n",
            "Epoch 30/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.2539 - val_accuracy: 0.9620\n",
            "Epoch 31/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.2815 - val_accuracy: 0.9577\n",
            "Epoch 32/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.2615 - val_accuracy: 0.9617\n",
            "Epoch 33/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.2715 - val_accuracy: 0.9592\n",
            "Epoch 34/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0178 - accuracy: 0.9943 - val_loss: 0.2765 - val_accuracy: 0.9623\n",
            "Epoch 35/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 0.2690 - val_accuracy: 0.9630\n",
            "Epoch 36/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.2569 - val_accuracy: 0.9634\n",
            "Epoch 37/50\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.2650 - val_accuracy: 0.9618\n",
            "Epoch 38/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0163 - accuracy: 0.9948 - val_loss: 0.2654 - val_accuracy: 0.9617\n",
            "Epoch 39/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0144 - accuracy: 0.9953 - val_loss: 0.2671 - val_accuracy: 0.9643\n",
            "Epoch 40/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.2614 - val_accuracy: 0.9633\n",
            "Epoch 41/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.2678 - val_accuracy: 0.9641\n",
            "Epoch 42/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0142 - accuracy: 0.9954 - val_loss: 0.3103 - val_accuracy: 0.9617\n",
            "Epoch 43/50\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.3226 - val_accuracy: 0.9604\n",
            "Epoch 44/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0157 - accuracy: 0.9950 - val_loss: 0.2716 - val_accuracy: 0.9636\n",
            "Epoch 45/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.2769 - val_accuracy: 0.9642\n",
            "Epoch 46/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.2754 - val_accuracy: 0.9637\n",
            "Epoch 47/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0135 - accuracy: 0.9956 - val_loss: 0.2988 - val_accuracy: 0.9624\n",
            "Epoch 48/50\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0128 - accuracy: 0.9960 - val_loss: 0.3381 - val_accuracy: 0.9591\n",
            "Epoch 49/50\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 0.3011 - val_accuracy: 0.9638\n",
            "Epoch 50/50\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.3154 - val_accuracy: 0.9610\n"
          ]
        }
      ],
      "source": [
        "# Training model with generator\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "training_generator = data_generator_double_visual_field(\n",
        "    x_train_left, x_train_right, y_train_final, batch_size\n",
        ")\n",
        "testing_generator = data_generator_double_visual_field(\n",
        "    x_test_left, x_test_right, y_test_final, batch_size\n",
        ")\n",
        "\n",
        "model_t1.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "history_model_t1 = model_t1.fit(\n",
        "    training_generator,\n",
        "    steps_per_epoch=len(y_train_final) // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=testing_generator,\n",
        "    validation_steps=len(y_test_final) // batch_size,\n",
        ")\n",
        "# history_model_t1 = model_t1.fit(\n",
        "#     [x_train_left, x_train_right],\n",
        "#     y_train_final,\n",
        "#     epochs=epochs,\n",
        "#     validation_data=([x_test_left, x_test_right], y_test_final),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_t1 = keras.models.load_model('models/model_t1.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_double_visual_field_mnist_nn_execution(\n",
        "    model_t1,\n",
        "    max_neurons=300,\n",
        "    weight_plot_threshold=0.8,\n",
        "    x_data=x_test,\n",
        "    left_vf_digit=(6, 1),\n",
        "    right_vf_digit=(61, 0.5),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.8457 - accuracy: 0.4611 - val_loss: 1.5216 - val_accuracy: 0.5859\n",
            "Epoch 2/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3918 - accuracy: 0.6018 - val_loss: 1.2628 - val_accuracy: 0.6329\n",
            "Epoch 3/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.2252 - accuracy: 0.6285 - val_loss: 1.1478 - val_accuracy: 0.6483\n",
            "Epoch 4/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.1444 - accuracy: 0.6414 - val_loss: 1.0878 - val_accuracy: 0.6570\n",
            "Epoch 5/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.1004 - accuracy: 0.6486 - val_loss: 1.0508 - val_accuracy: 0.6654\n",
            "Epoch 6/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0744 - accuracy: 0.6537 - val_loss: 1.0288 - val_accuracy: 0.6703\n",
            "Epoch 7/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.0581 - accuracy: 0.6573 - val_loss: 1.0132 - val_accuracy: 0.6732\n",
            "Epoch 8/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0477 - accuracy: 0.6594 - val_loss: 1.0075 - val_accuracy: 0.6750\n",
            "Epoch 9/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.0406 - accuracy: 0.6609 - val_loss: 1.0036 - val_accuracy: 0.6737\n",
            "Epoch 10/50\n",
            "937/937 [==============================] - 5s 5ms/step - loss: 1.0356 - accuracy: 0.6622 - val_loss: 0.9983 - val_accuracy: 0.6757\n",
            "Epoch 11/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.0323 - accuracy: 0.6627 - val_loss: 0.9938 - val_accuracy: 0.6784\n",
            "Epoch 12/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.0299 - accuracy: 0.6633 - val_loss: 0.9911 - val_accuracy: 0.6784\n",
            "Epoch 13/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.0276 - accuracy: 0.6637 - val_loss: 0.9874 - val_accuracy: 0.6796\n",
            "Epoch 14/50\n",
            "937/937 [==============================] - 6s 7ms/step - loss: 1.0264 - accuracy: 0.6642 - val_loss: 0.9883 - val_accuracy: 0.6804\n",
            "Epoch 15/50\n",
            "937/937 [==============================] - 7s 8ms/step - loss: 1.0254 - accuracy: 0.6644 - val_loss: 0.9886 - val_accuracy: 0.6801\n",
            "Epoch 16/50\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 1.0248 - accuracy: 0.6645 - val_loss: 0.9908 - val_accuracy: 0.6803\n",
            "Epoch 17/50\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 1.0238 - accuracy: 0.6647 - val_loss: 0.9951 - val_accuracy: 0.6782\n",
            "Epoch 18/50\n",
            "937/937 [==============================] - 6s 6ms/step - loss: 1.0235 - accuracy: 0.6648 - val_loss: 0.9911 - val_accuracy: 0.6798\n",
            "Epoch 19/50\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 1.0225 - accuracy: 0.6650 - val_loss: 0.9907 - val_accuracy: 0.6801\n",
            "Epoch 20/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.0224 - accuracy: 0.6651 - val_loss: 0.9878 - val_accuracy: 0.6800\n",
            "Epoch 21/50\n",
            "937/937 [==============================] - 7s 8ms/step - loss: 1.0220 - accuracy: 0.6654 - val_loss: 0.9971 - val_accuracy: 0.6759\n",
            "Epoch 22/50\n",
            "937/937 [==============================] - 5s 6ms/step - loss: 1.0221 - accuracy: 0.6655 - val_loss: 0.9883 - val_accuracy: 0.6797\n",
            "Epoch 23/50\n",
            "937/937 [==============================] - 5s 6ms/step - loss: 1.0217 - accuracy: 0.6656 - val_loss: 0.9878 - val_accuracy: 0.6800\n",
            "Epoch 24/50\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 1.0214 - accuracy: 0.6655 - val_loss: 0.9965 - val_accuracy: 0.6770\n",
            "Epoch 25/50\n",
            "937/937 [==============================] - 7s 7ms/step - loss: 1.0211 - accuracy: 0.6655 - val_loss: 0.9938 - val_accuracy: 0.6786\n",
            "Epoch 26/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.0214 - accuracy: 0.6653 - val_loss: 0.9885 - val_accuracy: 0.6801\n",
            "Epoch 27/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.0212 - accuracy: 0.6653 - val_loss: 0.9941 - val_accuracy: 0.6779\n",
            "Epoch 28/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.0214 - accuracy: 0.6652 - val_loss: 0.9956 - val_accuracy: 0.6773\n",
            "Epoch 29/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.0210 - accuracy: 0.6653 - val_loss: 0.9903 - val_accuracy: 0.6795\n",
            "Epoch 30/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.0207 - accuracy: 0.6653 - val_loss: 0.9938 - val_accuracy: 0.6778\n",
            "Epoch 31/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.0210 - accuracy: 0.6652 - val_loss: 0.9984 - val_accuracy: 0.6760\n",
            "Epoch 32/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.0206 - accuracy: 0.6654 - val_loss: 0.9887 - val_accuracy: 0.6791\n",
            "Epoch 33/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.0206 - accuracy: 0.6655 - val_loss: 0.9932 - val_accuracy: 0.6770\n",
            "Epoch 34/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.0207 - accuracy: 0.6654 - val_loss: 0.9887 - val_accuracy: 0.6797\n",
            "Epoch 35/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.0207 - accuracy: 0.6653 - val_loss: 0.9822 - val_accuracy: 0.6820\n",
            "Epoch 36/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.0206 - accuracy: 0.6654 - val_loss: 0.9878 - val_accuracy: 0.6795\n",
            "Epoch 37/50\n",
            "937/937 [==============================] - 5s 5ms/step - loss: 1.0203 - accuracy: 0.6655 - val_loss: 0.9867 - val_accuracy: 0.6798\n",
            "Epoch 38/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.0202 - accuracy: 0.6655 - val_loss: 0.9842 - val_accuracy: 0.6805\n",
            "Epoch 39/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0204 - accuracy: 0.6656 - val_loss: 0.9841 - val_accuracy: 0.6817\n",
            "Epoch 40/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.0200 - accuracy: 0.6655 - val_loss: 0.9838 - val_accuracy: 0.6808\n",
            "Epoch 41/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0204 - accuracy: 0.6655 - val_loss: 0.9803 - val_accuracy: 0.6804\n",
            "Epoch 42/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0208 - accuracy: 0.6655 - val_loss: 0.9829 - val_accuracy: 0.6807\n",
            "Epoch 43/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0201 - accuracy: 0.6658 - val_loss: 0.9825 - val_accuracy: 0.6808\n",
            "Epoch 44/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0202 - accuracy: 0.6656 - val_loss: 0.9825 - val_accuracy: 0.6801\n",
            "Epoch 45/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.0206 - accuracy: 0.6655 - val_loss: 0.9786 - val_accuracy: 0.6807\n",
            "Epoch 46/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.0205 - accuracy: 0.6655 - val_loss: 0.9851 - val_accuracy: 0.6808\n",
            "Epoch 47/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.0205 - accuracy: 0.6654 - val_loss: 0.9841 - val_accuracy: 0.6806\n",
            "Epoch 48/50\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 1.0201 - accuracy: 0.6655 - val_loss: 0.9863 - val_accuracy: 0.6790\n",
            "Epoch 49/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0203 - accuracy: 0.6655 - val_loss: 0.9820 - val_accuracy: 0.6787\n",
            "Epoch 50/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0202 - accuracy: 0.6654 - val_loss: 0.9854 - val_accuracy: 0.6779\n",
            "Epoch 1/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.7973 - accuracy: 0.4963 - val_loss: 1.4486 - val_accuracy: 0.6343\n",
            "Epoch 2/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.3141 - accuracy: 0.6440 - val_loss: 1.1856 - val_accuracy: 0.6654\n",
            "Epoch 3/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.1462 - accuracy: 0.6624 - val_loss: 1.0714 - val_accuracy: 0.6782\n",
            "Epoch 4/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.0675 - accuracy: 0.6730 - val_loss: 1.0144 - val_accuracy: 0.6852\n",
            "Epoch 5/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.0237 - accuracy: 0.6799 - val_loss: 0.9827 - val_accuracy: 0.6897\n",
            "Epoch 6/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 0.9968 - accuracy: 0.6845 - val_loss: 0.9518 - val_accuracy: 0.6975\n",
            "Epoch 7/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 0.9791 - accuracy: 0.6877 - val_loss: 0.9399 - val_accuracy: 0.7001\n",
            "Epoch 8/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9674 - accuracy: 0.6901 - val_loss: 0.9322 - val_accuracy: 0.6998\n",
            "Epoch 9/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9589 - accuracy: 0.6920 - val_loss: 0.9254 - val_accuracy: 0.7030\n",
            "Epoch 10/50\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 0.9529 - accuracy: 0.6933 - val_loss: 0.9153 - val_accuracy: 0.7040\n",
            "Epoch 11/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 0.9485 - accuracy: 0.6945 - val_loss: 0.9166 - val_accuracy: 0.7038\n",
            "Epoch 12/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9456 - accuracy: 0.6953 - val_loss: 0.9117 - val_accuracy: 0.7053\n",
            "Epoch 13/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9423 - accuracy: 0.6964 - val_loss: 0.9116 - val_accuracy: 0.7053\n",
            "Epoch 14/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9405 - accuracy: 0.6970 - val_loss: 0.9067 - val_accuracy: 0.7061\n",
            "Epoch 15/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9394 - accuracy: 0.6972 - val_loss: 0.9057 - val_accuracy: 0.7066\n",
            "Epoch 16/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9386 - accuracy: 0.6972 - val_loss: 0.9000 - val_accuracy: 0.7078\n",
            "Epoch 17/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9372 - accuracy: 0.6974 - val_loss: 0.9026 - val_accuracy: 0.7077\n",
            "Epoch 18/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9369 - accuracy: 0.6972 - val_loss: 0.8994 - val_accuracy: 0.7081\n",
            "Epoch 19/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9359 - accuracy: 0.6977 - val_loss: 0.8979 - val_accuracy: 0.7080\n",
            "Epoch 20/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9351 - accuracy: 0.6982 - val_loss: 0.9005 - val_accuracy: 0.7078\n",
            "Epoch 21/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9349 - accuracy: 0.6982 - val_loss: 0.9047 - val_accuracy: 0.7043\n",
            "Epoch 22/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9350 - accuracy: 0.6981 - val_loss: 0.9075 - val_accuracy: 0.7034\n",
            "Epoch 23/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9342 - accuracy: 0.6984 - val_loss: 0.9098 - val_accuracy: 0.7033\n",
            "Epoch 24/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9338 - accuracy: 0.6986 - val_loss: 0.9046 - val_accuracy: 0.7061\n",
            "Epoch 25/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9338 - accuracy: 0.6986 - val_loss: 0.8991 - val_accuracy: 0.7061\n",
            "Epoch 26/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9338 - accuracy: 0.6986 - val_loss: 0.9078 - val_accuracy: 0.7029\n",
            "Epoch 27/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9335 - accuracy: 0.6988 - val_loss: 0.9034 - val_accuracy: 0.7048\n",
            "Epoch 28/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9336 - accuracy: 0.6988 - val_loss: 0.9009 - val_accuracy: 0.7055\n",
            "Epoch 29/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9334 - accuracy: 0.6989 - val_loss: 0.9091 - val_accuracy: 0.7037\n",
            "Epoch 30/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9328 - accuracy: 0.6991 - val_loss: 0.9016 - val_accuracy: 0.7044\n",
            "Epoch 31/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9333 - accuracy: 0.6990 - val_loss: 0.9004 - val_accuracy: 0.7061\n",
            "Epoch 32/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9334 - accuracy: 0.6989 - val_loss: 0.9032 - val_accuracy: 0.7043\n",
            "Epoch 33/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9331 - accuracy: 0.6992 - val_loss: 0.9061 - val_accuracy: 0.7031\n",
            "Epoch 34/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9331 - accuracy: 0.6990 - val_loss: 0.9012 - val_accuracy: 0.7043\n",
            "Epoch 35/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9332 - accuracy: 0.6991 - val_loss: 0.9076 - val_accuracy: 0.7028\n",
            "Epoch 36/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9329 - accuracy: 0.6992 - val_loss: 0.9134 - val_accuracy: 0.6997\n",
            "Epoch 37/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9323 - accuracy: 0.6995 - val_loss: 0.9034 - val_accuracy: 0.7039\n",
            "Epoch 38/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 0.9322 - accuracy: 0.6996 - val_loss: 0.9034 - val_accuracy: 0.7036\n",
            "Epoch 39/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9326 - accuracy: 0.6994 - val_loss: 0.9028 - val_accuracy: 0.7038\n",
            "Epoch 40/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9323 - accuracy: 0.6995 - val_loss: 0.8940 - val_accuracy: 0.7068\n",
            "Epoch 41/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9329 - accuracy: 0.6995 - val_loss: 0.8998 - val_accuracy: 0.7057\n",
            "Epoch 42/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9329 - accuracy: 0.6994 - val_loss: 0.8995 - val_accuracy: 0.7067\n",
            "Epoch 43/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9325 - accuracy: 0.6996 - val_loss: 0.8953 - val_accuracy: 0.7079\n",
            "Epoch 44/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9323 - accuracy: 0.6997 - val_loss: 0.8963 - val_accuracy: 0.7079\n",
            "Epoch 45/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9328 - accuracy: 0.6995 - val_loss: 0.8989 - val_accuracy: 0.7068\n",
            "Epoch 46/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9328 - accuracy: 0.6995 - val_loss: 0.8886 - val_accuracy: 0.7096\n",
            "Epoch 47/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9324 - accuracy: 0.6996 - val_loss: 0.8933 - val_accuracy: 0.7085\n",
            "Epoch 48/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9327 - accuracy: 0.6996 - val_loss: 0.8955 - val_accuracy: 0.7082\n",
            "Epoch 49/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9327 - accuracy: 0.6995 - val_loss: 0.8963 - val_accuracy: 0.7074\n",
            "Epoch 50/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 0.9328 - accuracy: 0.6994 - val_loss: 0.8919 - val_accuracy: 0.7082\n",
            "Epoch 1/50\n",
            "937/937 [==============================] - 3s 2ms/step - loss: 2.0900 - accuracy: 0.3176 - val_loss: 1.9114 - val_accuracy: 0.4408\n",
            "Epoch 2/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.8169 - accuracy: 0.4410 - val_loss: 1.7192 - val_accuracy: 0.4652\n",
            "Epoch 3/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.6836 - accuracy: 0.4563 - val_loss: 1.6194 - val_accuracy: 0.4761\n",
            "Epoch 4/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.6097 - accuracy: 0.4661 - val_loss: 1.5592 - val_accuracy: 0.4836\n",
            "Epoch 5/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.5646 - accuracy: 0.4723 - val_loss: 1.5226 - val_accuracy: 0.4869\n",
            "Epoch 6/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.5347 - accuracy: 0.4771 - val_loss: 1.4971 - val_accuracy: 0.4894\n",
            "Epoch 7/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.5140 - accuracy: 0.4813 - val_loss: 1.4768 - val_accuracy: 0.4950\n",
            "Epoch 8/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4988 - accuracy: 0.4841 - val_loss: 1.4630 - val_accuracy: 0.4984\n",
            "Epoch 9/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4878 - accuracy: 0.4863 - val_loss: 1.4563 - val_accuracy: 0.4987\n",
            "Epoch 10/50\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 1.4796 - accuracy: 0.4887 - val_loss: 1.4458 - val_accuracy: 0.5019\n",
            "Epoch 11/50\n",
            "937/937 [==============================] - 5s 5ms/step - loss: 1.4731 - accuracy: 0.4906 - val_loss: 1.4444 - val_accuracy: 0.5018\n",
            "Epoch 12/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4682 - accuracy: 0.4919 - val_loss: 1.4374 - val_accuracy: 0.5035\n",
            "Epoch 13/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4638 - accuracy: 0.4930 - val_loss: 1.4347 - val_accuracy: 0.5036\n",
            "Epoch 14/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4606 - accuracy: 0.4936 - val_loss: 1.4295 - val_accuracy: 0.5062\n",
            "Epoch 15/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4584 - accuracy: 0.4940 - val_loss: 1.4299 - val_accuracy: 0.5053\n",
            "Epoch 16/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4564 - accuracy: 0.4945 - val_loss: 1.4294 - val_accuracy: 0.5030\n",
            "Epoch 17/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.4543 - accuracy: 0.4951 - val_loss: 1.4254 - val_accuracy: 0.5054\n",
            "Epoch 18/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4531 - accuracy: 0.4954 - val_loss: 1.4244 - val_accuracy: 0.5058\n",
            "Epoch 19/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.4513 - accuracy: 0.4958 - val_loss: 1.4218 - val_accuracy: 0.5076\n",
            "Epoch 20/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4504 - accuracy: 0.4959 - val_loss: 1.4202 - val_accuracy: 0.5078\n",
            "Epoch 21/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.4495 - accuracy: 0.4961 - val_loss: 1.4242 - val_accuracy: 0.5050\n",
            "Epoch 22/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4491 - accuracy: 0.4964 - val_loss: 1.4295 - val_accuracy: 0.5027\n",
            "Epoch 23/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4482 - accuracy: 0.4967 - val_loss: 1.4264 - val_accuracy: 0.5040\n",
            "Epoch 24/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4475 - accuracy: 0.4967 - val_loss: 1.4219 - val_accuracy: 0.5064\n",
            "Epoch 25/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4468 - accuracy: 0.4968 - val_loss: 1.4227 - val_accuracy: 0.5061\n",
            "Epoch 26/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4470 - accuracy: 0.4968 - val_loss: 1.4301 - val_accuracy: 0.5017\n",
            "Epoch 27/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.4463 - accuracy: 0.4970 - val_loss: 1.4218 - val_accuracy: 0.5039\n",
            "Epoch 28/50\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 1.4462 - accuracy: 0.4969 - val_loss: 1.4227 - val_accuracy: 0.5049\n",
            "Epoch 29/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4455 - accuracy: 0.4973 - val_loss: 1.4286 - val_accuracy: 0.5031\n",
            "Epoch 30/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4452 - accuracy: 0.4974 - val_loss: 1.4264 - val_accuracy: 0.5037\n",
            "Epoch 31/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4453 - accuracy: 0.4972 - val_loss: 1.4237 - val_accuracy: 0.5048\n",
            "Epoch 32/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4448 - accuracy: 0.4975 - val_loss: 1.4263 - val_accuracy: 0.5031\n",
            "Epoch 33/50\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 1.4448 - accuracy: 0.4974 - val_loss: 1.4270 - val_accuracy: 0.5030\n",
            "Epoch 34/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4444 - accuracy: 0.4975 - val_loss: 1.4247 - val_accuracy: 0.5035\n",
            "Epoch 35/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4445 - accuracy: 0.4974 - val_loss: 1.4223 - val_accuracy: 0.5042\n",
            "Epoch 36/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.4442 - accuracy: 0.4977 - val_loss: 1.4306 - val_accuracy: 0.5010\n",
            "Epoch 37/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4439 - accuracy: 0.4977 - val_loss: 1.4206 - val_accuracy: 0.5048\n",
            "Epoch 38/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4440 - accuracy: 0.4978 - val_loss: 1.4233 - val_accuracy: 0.5039\n",
            "Epoch 39/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4438 - accuracy: 0.4979 - val_loss: 1.4212 - val_accuracy: 0.5053\n",
            "Epoch 40/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4436 - accuracy: 0.4978 - val_loss: 1.4139 - val_accuracy: 0.5075\n",
            "Epoch 41/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4437 - accuracy: 0.4977 - val_loss: 1.4188 - val_accuracy: 0.5065\n",
            "Epoch 42/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4441 - accuracy: 0.4975 - val_loss: 1.4137 - val_accuracy: 0.5088\n",
            "Epoch 43/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4433 - accuracy: 0.4978 - val_loss: 1.4154 - val_accuracy: 0.5080\n",
            "Epoch 44/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.4431 - accuracy: 0.4978 - val_loss: 1.4129 - val_accuracy: 0.5089\n",
            "Epoch 45/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4435 - accuracy: 0.4977 - val_loss: 1.4150 - val_accuracy: 0.5079\n",
            "Epoch 46/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4432 - accuracy: 0.4978 - val_loss: 1.4146 - val_accuracy: 0.5082\n",
            "Epoch 47/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4436 - accuracy: 0.4977 - val_loss: 1.4117 - val_accuracy: 0.5092\n",
            "Epoch 48/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4431 - accuracy: 0.4979 - val_loss: 1.4111 - val_accuracy: 0.5085\n",
            "Epoch 49/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4433 - accuracy: 0.4978 - val_loss: 1.4142 - val_accuracy: 0.5086\n",
            "Epoch 50/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.4430 - accuracy: 0.4980 - val_loss: 1.4116 - val_accuracy: 0.5103\n",
            "Epoch 1/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 2.0743 - accuracy: 0.3270 - val_loss: 1.8975 - val_accuracy: 0.4436\n",
            "Epoch 2/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.7927 - accuracy: 0.4715 - val_loss: 1.6992 - val_accuracy: 0.4994\n",
            "Epoch 3/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.6466 - accuracy: 0.5010 - val_loss: 1.5859 - val_accuracy: 0.5166\n",
            "Epoch 4/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.5595 - accuracy: 0.5147 - val_loss: 1.5143 - val_accuracy: 0.5285\n",
            "Epoch 5/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.5036 - accuracy: 0.5232 - val_loss: 1.4695 - val_accuracy: 0.5320\n",
            "Epoch 6/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4656 - accuracy: 0.5289 - val_loss: 1.4306 - val_accuracy: 0.5402\n",
            "Epoch 7/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4387 - accuracy: 0.5325 - val_loss: 1.4092 - val_accuracy: 0.5430\n",
            "Epoch 8/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4191 - accuracy: 0.5355 - val_loss: 1.3916 - val_accuracy: 0.5466\n",
            "Epoch 9/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.4047 - accuracy: 0.5371 - val_loss: 1.3805 - val_accuracy: 0.5480\n",
            "Epoch 10/50\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 1.3936 - accuracy: 0.5386 - val_loss: 1.3663 - val_accuracy: 0.5530\n",
            "Epoch 11/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3848 - accuracy: 0.5401 - val_loss: 1.3638 - val_accuracy: 0.5514\n",
            "Epoch 12/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3783 - accuracy: 0.5411 - val_loss: 1.3564 - val_accuracy: 0.5542\n",
            "Epoch 13/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3722 - accuracy: 0.5423 - val_loss: 1.3544 - val_accuracy: 0.5539\n",
            "Epoch 14/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3677 - accuracy: 0.5435 - val_loss: 1.3452 - val_accuracy: 0.5568\n",
            "Epoch 15/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.3646 - accuracy: 0.5439 - val_loss: 1.3428 - val_accuracy: 0.5577\n",
            "Epoch 16/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3618 - accuracy: 0.5444 - val_loss: 1.3405 - val_accuracy: 0.5584\n",
            "Epoch 17/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3587 - accuracy: 0.5451 - val_loss: 1.3405 - val_accuracy: 0.5583\n",
            "Epoch 18/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3570 - accuracy: 0.5457 - val_loss: 1.3363 - val_accuracy: 0.5593\n",
            "Epoch 19/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.3547 - accuracy: 0.5461 - val_loss: 1.3335 - val_accuracy: 0.5603\n",
            "Epoch 20/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3530 - accuracy: 0.5464 - val_loss: 1.3345 - val_accuracy: 0.5599\n",
            "Epoch 21/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3518 - accuracy: 0.5465 - val_loss: 1.3378 - val_accuracy: 0.5578\n",
            "Epoch 22/50\n",
            "937/937 [==============================] - 2s 3ms/step - loss: 1.3511 - accuracy: 0.5463 - val_loss: 1.3386 - val_accuracy: 0.5579\n",
            "Epoch 23/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3496 - accuracy: 0.5466 - val_loss: 1.3400 - val_accuracy: 0.5557\n",
            "Epoch 24/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.3485 - accuracy: 0.5468 - val_loss: 1.3335 - val_accuracy: 0.5604\n",
            "Epoch 25/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3477 - accuracy: 0.5468 - val_loss: 1.3375 - val_accuracy: 0.5562\n",
            "Epoch 26/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.3473 - accuracy: 0.5468 - val_loss: 1.3318 - val_accuracy: 0.5584\n",
            "Epoch 27/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3467 - accuracy: 0.5471 - val_loss: 1.3325 - val_accuracy: 0.5594\n",
            "Epoch 28/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3463 - accuracy: 0.5474 - val_loss: 1.3383 - val_accuracy: 0.5567\n",
            "Epoch 29/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3455 - accuracy: 0.5477 - val_loss: 1.3335 - val_accuracy: 0.5580\n",
            "Epoch 30/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3449 - accuracy: 0.5477 - val_loss: 1.3307 - val_accuracy: 0.5598\n",
            "Epoch 31/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3450 - accuracy: 0.5478 - val_loss: 1.3302 - val_accuracy: 0.5592\n",
            "Epoch 32/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3446 - accuracy: 0.5477 - val_loss: 1.3326 - val_accuracy: 0.5581\n",
            "Epoch 33/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3441 - accuracy: 0.5478 - val_loss: 1.3291 - val_accuracy: 0.5600\n",
            "Epoch 34/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3438 - accuracy: 0.5479 - val_loss: 1.3349 - val_accuracy: 0.5581\n",
            "Epoch 35/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3437 - accuracy: 0.5479 - val_loss: 1.3399 - val_accuracy: 0.5561\n",
            "Epoch 36/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3431 - accuracy: 0.5481 - val_loss: 1.3315 - val_accuracy: 0.5584\n",
            "Epoch 37/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3427 - accuracy: 0.5482 - val_loss: 1.3303 - val_accuracy: 0.5586\n",
            "Epoch 38/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3423 - accuracy: 0.5482 - val_loss: 1.3317 - val_accuracy: 0.5581\n",
            "Epoch 39/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3424 - accuracy: 0.5481 - val_loss: 1.3274 - val_accuracy: 0.5582\n",
            "Epoch 40/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3424 - accuracy: 0.5480 - val_loss: 1.3275 - val_accuracy: 0.5586\n",
            "Epoch 41/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.3425 - accuracy: 0.5478 - val_loss: 1.3260 - val_accuracy: 0.5597\n",
            "Epoch 42/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3426 - accuracy: 0.5477 - val_loss: 1.3232 - val_accuracy: 0.5588\n",
            "Epoch 43/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3419 - accuracy: 0.5480 - val_loss: 1.3242 - val_accuracy: 0.5600\n",
            "Epoch 44/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3418 - accuracy: 0.5481 - val_loss: 1.3264 - val_accuracy: 0.5582\n",
            "Epoch 45/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3421 - accuracy: 0.5479 - val_loss: 1.3178 - val_accuracy: 0.5613\n",
            "Epoch 46/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.3422 - accuracy: 0.5478 - val_loss: 1.3208 - val_accuracy: 0.5592\n",
            "Epoch 47/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3418 - accuracy: 0.5479 - val_loss: 1.3209 - val_accuracy: 0.5596\n",
            "Epoch 48/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3416 - accuracy: 0.5480 - val_loss: 1.3229 - val_accuracy: 0.5590\n",
            "Epoch 49/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3417 - accuracy: 0.5478 - val_loss: 1.3168 - val_accuracy: 0.5609\n",
            "Epoch 50/50\n",
            "937/937 [==============================] - 2s 2ms/step - loss: 1.3418 - accuracy: 0.5478 - val_loss: 1.3233 - val_accuracy: 0.5581\n",
            "Epoch 1/50\n",
            "937/937 [==============================] - 6s 6ms/step - loss: 2.1642 - accuracy: 0.2692 - val_loss: 2.0540 - val_accuracy: 0.3358\n",
            "Epoch 2/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.9869 - accuracy: 0.3574 - val_loss: 1.9285 - val_accuracy: 0.3706\n",
            "Epoch 3/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8930 - accuracy: 0.3844 - val_loss: 1.8549 - val_accuracy: 0.3929\n",
            "Epoch 4/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8360 - accuracy: 0.3999 - val_loss: 1.8093 - val_accuracy: 0.4074\n",
            "Epoch 5/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7986 - accuracy: 0.4098 - val_loss: 1.7761 - val_accuracy: 0.4177\n",
            "Epoch 6/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7723 - accuracy: 0.4174 - val_loss: 1.7529 - val_accuracy: 0.4235\n",
            "Epoch 7/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7532 - accuracy: 0.4228 - val_loss: 1.7343 - val_accuracy: 0.4291\n",
            "Epoch 8/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7387 - accuracy: 0.4269 - val_loss: 1.7225 - val_accuracy: 0.4321\n",
            "Epoch 9/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7278 - accuracy: 0.4302 - val_loss: 1.7136 - val_accuracy: 0.4356\n",
            "Epoch 10/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7188 - accuracy: 0.4328 - val_loss: 1.7078 - val_accuracy: 0.4362\n",
            "Epoch 11/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7116 - accuracy: 0.4349 - val_loss: 1.7008 - val_accuracy: 0.4415\n",
            "Epoch 12/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7057 - accuracy: 0.4365 - val_loss: 1.6926 - val_accuracy: 0.4433\n",
            "Epoch 13/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7009 - accuracy: 0.4383 - val_loss: 1.6923 - val_accuracy: 0.4437\n",
            "Epoch 14/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.6967 - accuracy: 0.4399 - val_loss: 1.6878 - val_accuracy: 0.4455\n",
            "Epoch 15/50\n",
            "937/937 [==============================] - 5s 5ms/step - loss: 1.6932 - accuracy: 0.4412 - val_loss: 1.6866 - val_accuracy: 0.4462\n",
            "Epoch 16/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6901 - accuracy: 0.4423 - val_loss: 1.6799 - val_accuracy: 0.4478\n",
            "Epoch 17/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6876 - accuracy: 0.4431 - val_loss: 1.6789 - val_accuracy: 0.4487\n",
            "Epoch 18/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6853 - accuracy: 0.4442 - val_loss: 1.6763 - val_accuracy: 0.4492\n",
            "Epoch 19/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6832 - accuracy: 0.4449 - val_loss: 1.6777 - val_accuracy: 0.4483\n",
            "Epoch 20/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6815 - accuracy: 0.4457 - val_loss: 1.6752 - val_accuracy: 0.4500\n",
            "Epoch 21/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6799 - accuracy: 0.4462 - val_loss: 1.6708 - val_accuracy: 0.4527\n",
            "Epoch 22/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6786 - accuracy: 0.4465 - val_loss: 1.6688 - val_accuracy: 0.4534\n",
            "Epoch 23/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6773 - accuracy: 0.4470 - val_loss: 1.6687 - val_accuracy: 0.4536\n",
            "Epoch 24/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6763 - accuracy: 0.4473 - val_loss: 1.6705 - val_accuracy: 0.4524\n",
            "Epoch 25/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.6752 - accuracy: 0.4475 - val_loss: 1.6707 - val_accuracy: 0.4520\n",
            "Epoch 26/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6743 - accuracy: 0.4479 - val_loss: 1.6654 - val_accuracy: 0.4546\n",
            "Epoch 27/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6736 - accuracy: 0.4482 - val_loss: 1.6636 - val_accuracy: 0.4557\n",
            "Epoch 28/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6728 - accuracy: 0.4485 - val_loss: 1.6634 - val_accuracy: 0.4554\n",
            "Epoch 29/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.6721 - accuracy: 0.4489 - val_loss: 1.6648 - val_accuracy: 0.4553\n",
            "Epoch 30/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6715 - accuracy: 0.4491 - val_loss: 1.6659 - val_accuracy: 0.4520\n",
            "Epoch 31/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6709 - accuracy: 0.4493 - val_loss: 1.6646 - val_accuracy: 0.4547\n",
            "Epoch 32/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6705 - accuracy: 0.4495 - val_loss: 1.6638 - val_accuracy: 0.4550\n",
            "Epoch 33/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6700 - accuracy: 0.4497 - val_loss: 1.6661 - val_accuracy: 0.4540\n",
            "Epoch 34/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6695 - accuracy: 0.4499 - val_loss: 1.6625 - val_accuracy: 0.4560\n",
            "Epoch 35/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.6690 - accuracy: 0.4501 - val_loss: 1.6648 - val_accuracy: 0.4544\n",
            "Epoch 36/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6689 - accuracy: 0.4502 - val_loss: 1.6624 - val_accuracy: 0.4552\n",
            "Epoch 37/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6684 - accuracy: 0.4502 - val_loss: 1.6626 - val_accuracy: 0.4558\n",
            "Epoch 38/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.6681 - accuracy: 0.4504 - val_loss: 1.6616 - val_accuracy: 0.4575\n",
            "Epoch 39/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6678 - accuracy: 0.4505 - val_loss: 1.6585 - val_accuracy: 0.4569\n",
            "Epoch 40/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6675 - accuracy: 0.4506 - val_loss: 1.6608 - val_accuracy: 0.4556\n",
            "Epoch 41/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6674 - accuracy: 0.4507 - val_loss: 1.6604 - val_accuracy: 0.4575\n",
            "Epoch 42/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.6670 - accuracy: 0.4508 - val_loss: 1.6575 - val_accuracy: 0.4576\n",
            "Epoch 43/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.6669 - accuracy: 0.4508 - val_loss: 1.6629 - val_accuracy: 0.4565\n",
            "Epoch 44/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6666 - accuracy: 0.4509 - val_loss: 1.6590 - val_accuracy: 0.4573\n",
            "Epoch 45/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6666 - accuracy: 0.4509 - val_loss: 1.6633 - val_accuracy: 0.4546\n",
            "Epoch 46/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6663 - accuracy: 0.4510 - val_loss: 1.6625 - val_accuracy: 0.4563\n",
            "Epoch 47/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6662 - accuracy: 0.4511 - val_loss: 1.6623 - val_accuracy: 0.4564\n",
            "Epoch 48/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6662 - accuracy: 0.4512 - val_loss: 1.6643 - val_accuracy: 0.4555\n",
            "Epoch 49/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.6660 - accuracy: 0.4513 - val_loss: 1.6600 - val_accuracy: 0.4558\n",
            "Epoch 50/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.6659 - accuracy: 0.4514 - val_loss: 1.6611 - val_accuracy: 0.4562\n",
            "Epoch 1/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 2.2180 - accuracy: 0.2169 - val_loss: 2.1449 - val_accuracy: 0.2742\n",
            "Epoch 2/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 2.0961 - accuracy: 0.2901 - val_loss: 2.0531 - val_accuracy: 0.3111\n",
            "Epoch 3/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 2.0210 - accuracy: 0.3188 - val_loss: 1.9909 - val_accuracy: 0.3302\n",
            "Epoch 4/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.9694 - accuracy: 0.3361 - val_loss: 1.9465 - val_accuracy: 0.3435\n",
            "Epoch 5/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.9319 - accuracy: 0.3474 - val_loss: 1.9120 - val_accuracy: 0.3559\n",
            "Epoch 6/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.9037 - accuracy: 0.3562 - val_loss: 1.8848 - val_accuracy: 0.3664\n",
            "Epoch 7/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8820 - accuracy: 0.3627 - val_loss: 1.8660 - val_accuracy: 0.3713\n",
            "Epoch 8/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8647 - accuracy: 0.3674 - val_loss: 1.8493 - val_accuracy: 0.3733\n",
            "Epoch 9/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8509 - accuracy: 0.3714 - val_loss: 1.8356 - val_accuracy: 0.3777\n",
            "Epoch 10/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8395 - accuracy: 0.3749 - val_loss: 1.8235 - val_accuracy: 0.3827\n",
            "Epoch 11/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8299 - accuracy: 0.3777 - val_loss: 1.8174 - val_accuracy: 0.3823\n",
            "Epoch 12/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8217 - accuracy: 0.3798 - val_loss: 1.8059 - val_accuracy: 0.3854\n",
            "Epoch 13/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8148 - accuracy: 0.3818 - val_loss: 1.8017 - val_accuracy: 0.3863\n",
            "Epoch 14/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8088 - accuracy: 0.3833 - val_loss: 1.7943 - val_accuracy: 0.3873\n",
            "Epoch 15/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.8035 - accuracy: 0.3849 - val_loss: 1.7915 - val_accuracy: 0.3871\n",
            "Epoch 16/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7988 - accuracy: 0.3863 - val_loss: 1.7875 - val_accuracy: 0.3876\n",
            "Epoch 17/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7949 - accuracy: 0.3875 - val_loss: 1.7828 - val_accuracy: 0.3895\n",
            "Epoch 18/50\n",
            "937/937 [==============================] - 5s 5ms/step - loss: 1.7912 - accuracy: 0.3886 - val_loss: 1.7794 - val_accuracy: 0.3909\n",
            "Epoch 19/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7879 - accuracy: 0.3898 - val_loss: 1.7742 - val_accuracy: 0.3944\n",
            "Epoch 20/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7850 - accuracy: 0.3908 - val_loss: 1.7733 - val_accuracy: 0.3952\n",
            "Epoch 21/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7823 - accuracy: 0.3915 - val_loss: 1.7667 - val_accuracy: 0.3952\n",
            "Epoch 22/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.7800 - accuracy: 0.3922 - val_loss: 1.7654 - val_accuracy: 0.3971\n",
            "Epoch 23/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7777 - accuracy: 0.3928 - val_loss: 1.7669 - val_accuracy: 0.3958\n",
            "Epoch 24/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7758 - accuracy: 0.3934 - val_loss: 1.7635 - val_accuracy: 0.3964\n",
            "Epoch 25/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7740 - accuracy: 0.3940 - val_loss: 1.7613 - val_accuracy: 0.3966\n",
            "Epoch 26/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7724 - accuracy: 0.3944 - val_loss: 1.7561 - val_accuracy: 0.3994\n",
            "Epoch 27/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7709 - accuracy: 0.3947 - val_loss: 1.7539 - val_accuracy: 0.4013\n",
            "Epoch 28/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7694 - accuracy: 0.3951 - val_loss: 1.7509 - val_accuracy: 0.4002\n",
            "Epoch 29/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7682 - accuracy: 0.3954 - val_loss: 1.7515 - val_accuracy: 0.4010\n",
            "Epoch 30/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7670 - accuracy: 0.3957 - val_loss: 1.7549 - val_accuracy: 0.4001\n",
            "Epoch 31/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.7659 - accuracy: 0.3959 - val_loss: 1.7540 - val_accuracy: 0.3998\n",
            "Epoch 32/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7649 - accuracy: 0.3962 - val_loss: 1.7498 - val_accuracy: 0.4022\n",
            "Epoch 33/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7639 - accuracy: 0.3964 - val_loss: 1.7466 - val_accuracy: 0.4036\n",
            "Epoch 34/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7630 - accuracy: 0.3965 - val_loss: 1.7495 - val_accuracy: 0.4015\n",
            "Epoch 35/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.7621 - accuracy: 0.3968 - val_loss: 1.7446 - val_accuracy: 0.4031\n",
            "Epoch 36/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7614 - accuracy: 0.3971 - val_loss: 1.7467 - val_accuracy: 0.4023\n",
            "Epoch 37/50\n",
            "937/937 [==============================] - 4s 5ms/step - loss: 1.7608 - accuracy: 0.3975 - val_loss: 1.7456 - val_accuracy: 0.4021\n",
            "Epoch 38/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7600 - accuracy: 0.3977 - val_loss: 1.7445 - val_accuracy: 0.4013\n",
            "Epoch 39/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7593 - accuracy: 0.3978 - val_loss: 1.7436 - val_accuracy: 0.4018\n",
            "Epoch 40/50\n",
            "937/937 [==============================] - 3s 4ms/step - loss: 1.7588 - accuracy: 0.3979 - val_loss: 1.7441 - val_accuracy: 0.4013\n",
            "Epoch 41/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7583 - accuracy: 0.3980 - val_loss: 1.7440 - val_accuracy: 0.4020\n",
            "Epoch 42/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7577 - accuracy: 0.3982 - val_loss: 1.7420 - val_accuracy: 0.4029\n",
            "Epoch 43/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7573 - accuracy: 0.3982 - val_loss: 1.7436 - val_accuracy: 0.4015\n",
            "Epoch 44/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7568 - accuracy: 0.3984 - val_loss: 1.7411 - val_accuracy: 0.4022\n",
            "Epoch 45/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7564 - accuracy: 0.3985 - val_loss: 1.7397 - val_accuracy: 0.4032\n",
            "Epoch 46/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7559 - accuracy: 0.3987 - val_loss: 1.7388 - val_accuracy: 0.4044\n",
            "Epoch 47/50\n",
            "937/937 [==============================] - 3s 3ms/step - loss: 1.7556 - accuracy: 0.3988 - val_loss: 1.7387 - val_accuracy: 0.4037\n",
            "Epoch 48/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7551 - accuracy: 0.3989 - val_loss: 1.7382 - val_accuracy: 0.4027\n",
            "Epoch 49/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7549 - accuracy: 0.3989 - val_loss: 1.7377 - val_accuracy: 0.4027\n",
            "Epoch 50/50\n",
            "937/937 [==============================] - 4s 4ms/step - loss: 1.7545 - accuracy: 0.3990 - val_loss: 1.7359 - val_accuracy: 0.4038\n"
          ]
        }
      ],
      "source": [
        "# Training out models with generator\n",
        "\n",
        "output_models = generate_output_models(model=model_t1)\n",
        "epochs = 50\n",
        "batch_size = 256\n",
        "training_generator = None\n",
        "for out_model in output_models:\n",
        "    if is_double_visual_field_model(out_model):\n",
        "        training_generator = data_generator_double_visual_field(\n",
        "            x_train_left, x_train_right, y_train_final, batch_size\n",
        "        )\n",
        "        testing_generator = data_generator_double_visual_field(\n",
        "            x_test_left, x_test_right, y_test_final, batch_size\n",
        "        )\n",
        "    else:\n",
        "        training_generator = data_generator_single_visual_field(\n",
        "            x_train, y_train, batch_size\n",
        "        )\n",
        "        testing_generator = data_generator_single_visual_field(\n",
        "            x_test, y_test, batch_size\n",
        "        )\n",
        "\n",
        "    out_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    out_model.fit(\n",
        "        training_generator,\n",
        "        steps_per_epoch=len(y_train_final) // batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=testing_generator,\n",
        "        validation_steps=len(y_test_final) // batch_size\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAADyCAYAAAAoXEDEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm4ElEQVR4nO3de1xUdf4/8NdAMiCXQRAYQSBCkxQvu6REtoKKIpVpmN3zmrfQvHah/eZ1H1G67VqJoqlQri4bmWZtaykCWoml4i2LvKCiCF6SAVFQ4fP7w5+TE+cgA2eYOczr+Xicx8N5n9v7g7x5z5n5zBmNEEKAiIhIpRysnQAREVFTsJEREZGqsZEREZGqsZEREZGqsZEREZGqsZEREZGqsZEREZGqsZEREZGqsZEREZGqsZHZqNLSUjzxxBPw9vaGRqPB4sWLrZ0SkVXFxMQgJiam0fuGh4crmxDZDDYyhaSnp0Oj0WD37t2KHG/69On4+uuvkZSUhDVr1mDQoEH46quvMHfu3AYfg8VLtuxWzdxa7rrrLgQEBGDUqFE4c+aMVXIqLi7G3LlzsW/fvgZtr3TdU+PcZe0ESNq2bdswZMgQzJo1yxhbsmQJUlJSzGpmRLZu/vz5CAkJQVVVFfLy8pCeno5vv/0Whw4dgrOzs3G7b775xuK5FBcXY968ebj77rvRo0cPi5+PlMFGZqPOnTsHT09Pa6dBZHHx8fG4//77AQAvvvgi2rZti3feeQebNm3Ck08+adzOycnJWimSjeNLi83szJkzGDNmDPz8/KDVatGlSxesXr3auP7WSxVCCKSkpBhfdhk1ahRSUlIAwOTlGHNpNBpMnjwZmZmZ6Ny5M1xcXBAVFYWDBw8CAJYvX44OHTrA2dkZMTExOHHihMn+O3bswPDhwxEUFAStVovAwEBMnz4dV69erXOuW+dwdnZGeHg4NmzYgFGjRuHuu+822a62thaLFy9Gly5d4OzsDD8/P0yYMAGXLl0ye3ykfn/5y18AAMeOHTOJS71HdvLkSTz22GNwdXWFr6+v8SV5jUaDnJycOsc+fPgw+vbti9atWyMgIAALFy40rsvJyUHPnj0BAKNHjzbWWHp6uln5jxo1Cm5ubjh16hQeffRRuLm5ISAgwFi/Bw8eRL9+/eDq6org4GCsW7fOZP/ffvsNs2bNQteuXeHm5gYPDw/Ex8dj//79dc5lzvh37dqFQYMGQafToXXr1oiOjsZ3331n1thsFa/ImlFpaSkeeOABYzPx8fHB//73P4wdOxbl5eWYNm0a+vTpgzVr1uCFF17AgAEDMGLECABAaGgoiouLsWXLFqxZs6ZJeezYsQObNm1CYmIiACA5ORmPPvooXn31VSxduhQvvfQSLl26hIULF2LMmDHYtm2bcd/MzExcuXIFkyZNgre3N3744Qd88MEHOH36NDIzM43b/fe//8VTTz2Frl27Ijk5GZcuXcLYsWMREBBQJ58JEyYgPT0do0ePxssvv4zCwkIsWbIE+fn5+O6779CqVasmjZfU5daTpzZt2tS7XWVlJfr164ezZ89i6tSp0Ov1WLduHbKzsyW3v3TpEgYNGoSEhAQ8+eST+PTTT/Haa6+ha9euiI+Px3333Yf58+dj9uzZGD9+vLGhPvjgg2aPoaamBvHx8ejTpw8WLlyItWvXYvLkyXB1dcVf//pXPPfcc0hISEBqaipGjBiBqKgohISEAACOHz+OjRs3Yvjw4QgJCUFpaSmWL1+O6OhoHD58GP7+/maPf9u2bYiPj0dERATmzJkDBwcHpKWloV+/ftixYwd69epl9hhtiiBFpKWlCQDixx9/lN1m7Nixol27duLChQsm8aefflrodDpx5coVYwyASExMNNkuMTFRmPNfFh0dLbp06WISAyC0Wq0oLCw0xpYvXy4ACL1eL8rLy43xpKQkAcBk29tzvCU5OVloNBpx8uRJY6xr166iffv2oqKiwhjLyckRAERwcLAxtmPHDgFArF271uSYmzdvloxTy3GrZrZu3SrOnz8vioqKxKeffip8fHyEVqsVRUVFJttHR0eL6Oho4+N3331XABAbN240xq5evSrCwsIEAJGdnW2yLwDx8ccfG2PV1dVCr9eLYcOGGWM//vijACDS0tLMGsPtdT9y5EgBQLz11lvG2KVLl4SLi4vQaDQiIyPDGP/ll18EADFnzhxjrKqqStTU1Jicp7CwUGi1WjF//nyzx19bWys6duwo4uLiRG1trXHbK1euiJCQEDFgwIAGjdWW8aXFZiKEwPr16zF48GAIIXDhwgXjEhcXB4PBgL179zZLLv379zd5eS8yMhIAMGzYMLi7u9eJHz9+3BhzcXEx/ruyshIXLlzAgw8+CCEE8vPzAdx8w/zgwYMYMWIE3NzcjNtHR0eja9euJrlkZmZCp9NhwIABJj+TiIgIuLm5yT67ppYjNjYWPj4+CAwMxBNPPAFXV1ds2rQJ7du3r3e/zZs3IyAgAI899pgx5uzsjHHjxklu7+bmhueff9742MnJCb169TL5/VbSiy++aPy3p6cnOnXqBFdXV5P3/Tp16gRPT0+THLRaLRwcbv5prqmpwcWLF+Hm5oZOnTqZ/I1o6Pj37duHI0eO4Nlnn8XFixeNNVZZWYn+/ftj+/btqK2tVXz8zYkvLTaT8+fPo6ysDCtWrMCKFSsktzl37lyz5BIUFGTyWKfTAQACAwMl47e/V3Xq1CnMnj0bmzZtqvMelsFgAHDzdXsA6NChQ51zd+jQwaQYjxw5AoPBAF9fX8lcm+tnQtaTkpKCe++9FwaDAatXr8b27duh1WrvuN/JkycRGhpa571iqd87AGjfvn2dbdu0aYMDBw40PnkZzs7O8PHxMYnpdDrJHHQ6nUkt1dbW4r333sPSpUtRWFiImpoa4zpvb2/jvxs6/iNHjgAARo4cKZuvwWC440u5toyNrJncesbz/PPPy/5CdevWrVlycXR0NCsuhABw89nhgAED8Ntvv+G1115DWFgYXF1dcebMGYwaNapRz+pqa2vh6+uLtWvXSq7/4x8Danl69eplnLU4dOhQPPTQQ3j22WdRUFBgckXfVHf6/VZSY2sMAN566y28+eabGDNmDBYsWAAvLy84ODhg2rRpja4xAFi0aJHsRwqU/DlbAxtZM/Hx8YG7uztqamoQGxvbqGM0Zpaikg4ePIhff/0VH330kXESCgBs2bLFZLvg4GAAwNGjR+sc44+x0NBQbN26Fb179zZ52ZLsk6OjI5KTk9G3b18sWbIEr7/+uuy2wcHBOHz4MIQQJrUh9XvXUNauMQD49NNP0bdvX6xatcokXlZWhrZt2xofN3T8oaGhAAAPD49G/+2xdXyPrJk4Ojpi2LBhWL9+PQ4dOlRn/fnz5+94DFdXVwA3f6Gt4dazydufPQoh8N5775ls5+/vj/DwcHz88ce4fPmyMZ6bm2uc5n/Lk08+iZqaGixYsKDO+W7cuGG1sZL1xMTEoFevXli8eDGqqqpkt4uLi8OZM2ewadMmY6yqqgoffvhho89t7RoDbtbZH68SMzMz69ztpKHjj4iIQGhoKP7+97+b1OMtDfnbY+t4Raaw1atXY/PmzXXiU6dOxdtvv43s7GxERkZi3Lhx6Ny5M3777Tfs3bsXW7duxW+//VbvsSMiIgAAL7/8MuLi4uDo6Iinn37aIuOQEhYWhtDQUMyaNQtnzpyBh4cH1q9fL/l5r7feegtDhgxB7969MXr0aFy6dAlLlixBeHi4STFFR0djwoQJSE5Oxr59+zBw4EC0atUKR44cQWZmJt577z088cQTzTZGsg2vvPIKhg8fjvT0dEycOFFymwkTJmDJkiV45plnMHXqVLRr1w5r16413g2kMVdXoaGh8PT0RGpqKtzd3eHq6orIyEjj1Pjm8Oijj2L+/PkYPXo0HnzwQRw8eBBr167FPffcY7JdQ8fv4OCAlStXIj4+Hl26dMHo0aMREBCAM2fOIDs7Gx4eHvjiiy+abXwWYaXZki3OrWm4csutqcSlpaUiMTFRBAYGilatWgm9Xi/69+8vVqxYYXI8SEy/v3HjhpgyZYrw8fERGo3mjlPx5abf//G4hYWFAoBYtGiRSTw7O1sAEJmZmcbY4cOHRWxsrHBzcxNt27YV48aNE/v375ecspyRkSHCwsKEVqsV4eHhYtOmTWLYsGEiLCysTq4rVqwQERERwsXFRbi7u4uuXbuKV199VRQXF9c7RlKv+j6yUlNTI0JDQ0VoaKi4ceOGEKLu9HshhDh+/Lh45JFHhIuLi/Dx8REzZ84U69evFwBEXl6ecTupWhDi5lT52z8OIoQQn3/+uejcubO466677jgVX276vaura51t5XIIDg4WjzzyiPFxVVWVmDlzpmjXrp1wcXERvXv3Fjt37mzS+IUQIj8/XyQkJAhvb2+h1WpFcHCwePLJJ0VWVpbs+NRCI4QF3ukkktGjRw/4+PjUeV+NSCmLFy/G9OnTcfr0ackP4Ld09jh+vkdGFnH9+nXcuHHDJJaTk4P9+/c3+qs4iP7oj7dGq6qqwvLly9GxY0e7+CNu7+O/he+RkUWcOXMGsbGxeP755+Hv749ffvkFqamp0Ov1su95EJkrISEBQUFB6NGjBwwGA/71r3/hl19+kf04R0tj7+O/hY2MLKJNmzaIiIjAypUrcf78ebi6uuKRRx7B22+/bfKhTqKmiIuLw8qVK7F27VrU1NSgc+fOyMjIwFNPPWXt1JqFvY//Fr5HRkREqsb3yIiISNXYyIiISNUs9h5ZSkoKFi1ahJKSEnTv3h0ffPBBg77zpra2FsXFxXB3d7eJ28UQmUMIgYqKCvj7+xvvYN5Uja0lgPVE6tbgerLEh9MyMjKEk5OTWL16tfjpp5/EuHHjhKenpygtLb3jvkVFRfV+sJgLFzUsf/wuLWvUEuuJS0tZ7lRPFpnsERkZiZ49e2LJkiUAbj4rDAwMxJQpU+q9CShw8+sEPD09UVRUBA8PD6VTI7Ko8vJyBAYGoqyszPg1OE3RlFoCWE+kbg2tJ8VfWrx27Rr27NmDpKQkY8zBwQGxsbHYuXNnne2rq6tRXV1tfFxRUQHg5p2aWXikVkq8jGduLQGsJ2qZ7lRPik/2uHDhAmpqauDn52cS9/PzQ0lJSZ3tk5OTodPpjMsfv9yRyF6ZW0sA64nsk9VnLSYlJcFgMBiXoqIia6dEpFqsJ7JHir+02LZtWzg6OqK0tNQkXlpaCr1eX2d7rVbboK81J7I35tYSwHoi+6T4FZmTkxMiIiKQlZVljNXW1iIrKwtRUVFKn46oxWItETWMRT5HNmPGDIwcORL333+/8ZteKysrMXr0aEucjqjFYi0R3ZlFGtlTTz2F8+fPY/bs2SgpKUGPHj2wefPmOm9aE1H9WEtEd2ZzNw0uLy+HTqeDwWDgdGFSHVv7/bW1fIjM0dDfX6vPWiQiImoKNjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1xRvZ3LlzodFoTJawsDClT0PU4rGWiBrmLksctEuXLti6devvJ7nLIqdRtcrKSsl4Zmam7D6BgYGS8YiICMm4p6en2XnZqurqasn4yZMnJeMdOnSQPZaDg3peiGAt2Y5Dhw5Jxmtqasw+Vvfu3ZuaDt3GIlVx1113Qa/XW+LQRHaFtUR0ZxZ5anrkyBH4+/vjnnvuwXPPPYdTp05Z4jRELR5riejOFL8ii4yMRHp6Ojp16oSzZ89i3rx5+Mtf/oJDhw7B3d29zvbV1dUmLxuVl5crnRKRKplbSwDrieyT4o0sPj7e+O9u3bohMjISwcHB+OSTTzB27Ng62ycnJ2PevHlKp0GkeubWEsB6Ivtk8Xe9PT09ce+99+Lo0aOS65OSkmAwGIxLUVGRpVMiUqU71RLAeiL7ZPEpUJcvX8axY8fwwgsvSK7XarXQarWWTsNqqqqqJONLly41a3sAcHNzk4y3lNmJcjMTAWD58uWS8StXrkjGx48fL3ssLy8v8xKzEXeqJaDl15OSrl69KhlftWqVZHzmzJmS8evXr5t97m7duknGNRqN2ceSExUVJbtu+PDhkvH7779fMi73UratUPyKbNasWcjNzcWJEyfw/fff4/HHH4ejoyOeeeYZpU9F1KKxlogaRvErstOnT+OZZ57BxYsX4ePjg4ceegh5eXnw8fFR+lRELRpriahhFG9kGRkZSh+SyC6xlogaRj23OCAiIpLARkZERKrGG7cpQG7mHAB8+umnknG5GVM9e/aUPdbtnytqibZv3y67rqysTDL+6KOPSsbVOjORlCVXZwDw+OOPS8a//vprybiSMwr3799v8XPs27dPdl1qaqpkXO6m1FlZWbLHateunVl5WQKvyIiISNXYyIiISNXYyIiISNXYyIiISNXYyIiISNXYyIiISNU4/V4BZ8+elV134sQJs44VHR3dxGxs3/nz5yXj33//vew+9913n2Q8PDxckZxI3Xbt2iUZnzx5suw+u3fvNusckZGRkvFBgwaZdRwA6N+/v2S8sLBQdh8XFxfJuNxNw9evXy97rM2bN0vGf/75Z8n466+/Lnusjz76SHZdc+EVGRERqRobGRERqRobGRERqRobGRERqRobGRERqRpnLZqhsrJSMi4306c+jz32mGS8devWZh/LVsnNTvz444/NPpbczUydnJzMPha1PHIz9Pbs2SO7j9wNeuVmJ3755ZeScW9v7ztk13APPfSQYseKjY2VXTdu3DjJ+KpVqyTje/fuVSQnS+EVGRERqRobGRERqRobGRERqRobGRERqRobGRERqZrZsxa3b9+ORYsWYc+ePTh79iw2bNiAoUOHGtcLITBnzhx8+OGHKCsrQ+/evbFs2TJ07NhRybyt4ptvvpGMy31tOQD4+/tLxrt06aJITrbs5MmTkvHLly9Lxnv06CF7rG7duimRkk2x51qyZV999ZVkvE2bNs2cSdPIjQMAPvnkk2bMxPLMviKrrKxE9+7dkZKSIrl+4cKFeP/995Gamopdu3bB1dUVcXFxqKqqanKyRC0Ja4lIGWZfkcXHxyM+Pl5ynRACixcvxv/93/9hyJAhAG5+ZsjPzw8bN27E008/3bRsiVoQ1hKRMhR9j6ywsBAlJSUmH8TT6XSIjIzEzp07Jfeprq5GeXm5yUJk7xpTSwDrieyToo2spKQEAODn52cS9/PzM677o+TkZOh0OuMSGBioZEpEqtSYWgJYT2SfrD5rMSkpCQaDwbgUFRVZOyUi1WI9kT1StJHp9XoAQGlpqUm8tLTUuO6PtFotPDw8TBYie9eYWgJYT2SfFL1pcEhICPR6PbKysoxTqcvLy7Fr1y5MmjRJyVPZFAcH+ecD7u7uknFHR0dLpWMRN27ckIzv2LFDdp8ff/xRMi53s9ZbkxrIfmupsUJCQhQ7VmZmpmR8/Pjxip1DScePH5eMv/jii7L7VFRUmHWOiIgIs7ZvbmY3ssuXL+Po0aPGx4WFhdi3bx+8vLwQFBSEadOm4W9/+xs6duyIkJAQvPnmm/D39zf5fAwRsZaIlGJ2I9u9ezf69u1rfDxjxgwAwMiRI5Geno5XX30VlZWVGD9+PMrKyvDQQw9h8+bNcHZ2Vi5rohaAtUSkDLMbWUxMDIQQsus1Gg3mz5+P+fPnNykxopaOtUSkDKvPWiQiImoKNjIiIlI1RWctUl1HjhyRjK9Zs0YyXt/7H/fff78iOdVH7ka/J06ckIyfPn3a7HN07tzZ7H2I6iM3k/PgwYOy+yxbtkwyPnfuXMl4nz59JONhYWH1J2eGX3/9VXbdu+++KxlfsWKFYud/5JFHJOPJycmKncMSeEVGRESqxkZGRESqxkZGRESqxkZGRESqxkZGRESqxlmLZnjggQck44WFhbL7XL58WTIuNzuwvg/IFhQU1JOdMuTOL3d/xPrIfTV8//79zT4WUWMsWLBAdl1+fr5kPC8vTzL+zDPPSMa///572XOcPXtWMi43O/GFF16QPdbFixcl43K1GRQUJHus4cOHS8Znz54tGZe7Z6yt4BUZERGpGhsZERGpGhsZERGpGhsZERGpGhsZERGpGmctmqFdu3aS8Zdeekl2n5KSEsn47V+oeLvvvvtO9litW7eWjN/6BmEldOvWTTKemppq9rECAwMl43KzGYmU5u3tLbvuq6++kozL3VNx//79kvE///nPsueQm7VoMBgk4/XNDm7btq1kXO7vz9SpU2WP1dJqkFdkRESkamxkRESkamxkRESkamxkRESkamxkRESkamY3su3bt2Pw4MHw9/eHRqPBxo0bTdaPGjUKGo3GZBk0aJBS+RK1GKwlImWYPf2+srIS3bt3x5gxY5CQkCC5zaBBg5CWlmZ8rNVqG5+hCjg7O8uuu/vuu82Kx8bGKpBR4126dEkyLnczYb1eL3usgQMHKpJTS8Vasq7MzEzJuNzNeeU05mbeAQEBkvFly5bJ7hMTEyMZt/Ub+jYHsxtZfHw84uPj691Gq9XW+weOiFhLREqxyHtkOTk58PX1RadOnTBp0iSzn+EQ0U2sJaI7U/zOHoMGDUJCQgJCQkJw7NgxvPHGG4iPj8fOnTvh6OhYZ/vq6mpUV1cbH5eXlyudEpEqmVtLAOuJ7JPijezpp582/rtr167o1q0bQkNDkZOTI/mFisnJyZg3b57SaRCpnrm1BLCeyD5ZfPr9Pffcg7Zt28reWzApKQkGg8G4FBUVWTolIlW6Uy0BrCeyTxa/afDp06dx8eJF2RvuarVazsSyIbm5uZJxuZuZDhgwQPZYcjc5psa5Uy0BLb+evvnmG8n4ypUrJeNyMxOtbcqUKZLxwYMHN3MmLYPZjezy5csmzwgLCwuxb98+eHl5wcvLC/PmzcOwYcOg1+tx7NgxvPrqq+jQoQPi4uIUTZxI7VhLRMowu5Ht3r0bffv2NT6eMWMGAGDkyJFYtmwZDhw4gI8++ghlZWXw9/fHwIEDsWDBghb9LJGoMVhLRMowu5HFxMTIfjgWAL7++usmJURkL1hLRMrgvRaJiEjV2MiIiEjVLD5rkWzP4cOHZdfJfZ273PsyLi4uiuRELVdxcbFkPDU1VTL+4Ycfyh6rpKREMi43q1YuDsjPuJW7R2hERIRkXG4GIgD89NNPkvE5c+ZIxseMGSN7LB8fH9l19o5XZEREpGpsZEREpGpsZEREpGpsZEREpGpsZEREpGpsZEREpGqcfm+Hjhw5YvY+9957r2S8vhvYkv149tlnZddt27ZNMn7u3Dmzz+Ps7CwZHz58uGR81qxZsscKCQmRjDs5OUnG5W5AfOzYMdlzyLn9O+MaeixOv5fHKzIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1zlq0Q7d/K/EftWrVSjIeFRVlqXSoBZCb1QoAGRkZip2nY8eOknG5GwBv2LBB9lgnTpyQjB84cEAynp+fX39yZggICJCM33fffYqdw57wioyIiFSNjYyIiFSNjYyIiFSNjYyIiFSNjYyIiFTNrFmLycnJ+Oyzz/DLL7/AxcUFDz74IN555x106tTJuE1VVRVmzpyJjIwMVFdXIy4uDkuXLoWfn5/iyVP9du/eLRm/fPmy7D6urq6Scd5TUVktrZbmzp0ru+7atWuS8ZSUFMl4eXm57LEOHjwoGR8xYoR8clYkNztx+/btknGdTmfJdFoss67IcnNzkZiYiLy8PGzZsgXXr1/HwIEDUVlZadxm+vTp+OKLL5CZmYnc3FwUFxcjISFB8cSJ1Iy1RKQcs67INm/ebPI4PT0dvr6+2LNnD/r06QODwYBVq1Zh3bp16NevHwAgLS0N9913H/Ly8vDAAw8olzmRirGWiJTTpPfIDAYDAMDLywsAsGfPHly/fh2xsbHGbcLCwhAUFISdO3dKHqO6uhrl5eUmC5G9UaKWANYT2adGN7La2lpMmzYNvXv3Rnh4OACgpKQETk5O8PT0NNnWz88PJSUlksdJTk6GTqczLoGBgY1NiUiVlKolgPVE9qnRjSwxMRGHDh1q8u1nkpKSYDAYjEtRUVGTjkekNkrVEsB6IvvUqHstTp48GV9++SW2b9+O9u3bG+N6vR7Xrl1DWVmZyTPJ0tJS6PV6yWNptVpotdrGpEGkekrWEsB6IvtkViMTQmDKlCnYsGEDcnJy6nxVeEREBFq1aoWsrCwMGzYMAFBQUIBTp07xprNWIDf9XqPRyO5T381fpchNrQaAq1evSsY5xdi+aumtt96SjCcmJkrG67syLS4uloxv27bN/MQUcuv/R8qUKVMk46wBZZnVyBITE7Fu3Tp8/vnncHd3N75Wr9Pp4OLiAp1Oh7Fjx2LGjBnw8vKCh4cHpkyZgqioKM6yIroNa4lIOWY1smXLlgEAYmJiTOJpaWkYNWoUAOCf//wnHBwcMGzYMJMPcRLR71hLRMox+6XFO3F2dkZKSorsp/aJiLVEpCTea5GIiFSNjYyIiFStUdPvqeWSm9Eod7PW+u4y4evrKxkfOnSo2XlRyyN3Q92ZM2c2cyakdrwiIyIiVWMjIyIiVWMjIyIiVWMjIyIiVWMjIyIiVeOsRTKxd+9eyXh+fr5k/E9/+pPssaKjoxXJiYioPrwiIyIiVWMjIyIiVWMjIyIiVWMjIyIiVWMjIyIiVWMjIyIiVeP0+xbs4YcfloxnZ2fL7hMcHCwZ79mzp2Tc2dlZ9liOjo71ZEdEpAxekRERkaqxkRERkaqxkRERkaqxkRERkaqZ1ciSk5PRs2dPuLu7w9fXF0OHDkVBQYHJNjExMdBoNCbLxIkTFU2aSO1YS0TKMWvWYm5uLhITE9GzZ0/cuHEDb7zxBgYOHIjDhw/D1dXVuN24ceMwf/584+PWrVsrlzE1WFBQkGR85MiRzZwJ/RFriUg5ZjWyzZs3mzxOT0+Hr68v9uzZgz59+hjjrVu3hl6vVyZDohaItUSknCa9R2YwGAAAXl5eJvG1a9eibdu2CA8PR1JSEq5cudKU0xC1eKwlosZr9Aeia2trMW3aNPTu3Rvh4eHG+LPPPovg4GD4+/vjwIEDeO2111BQUIDPPvtM8jjV1dWorq42Pi4vL29sSkSqpFQtAawnslOikSZOnCiCg4NFUVFRvdtlZWUJAOLo0aOS6+fMmSMA1FkMBkNjUyOyGoPBYPbvr1K1JATriVqWhtZTo15anDx5Mr788ktkZ2ejffv29W4bGRkJADh69Kjk+qSkJBgMBuNSVFTUmJSIVEnJWgJYT2SfzHppUQiBKVOmYMOGDcjJyUFISMgd99m3bx8AoF27dpLrtVottFqtOWkQqZ4laglgPZF9MquRJSYmYt26dfj888/h7u6OkpISAIBOp4OLiwuOHTuGdevW4eGHH4a3tzcOHDiA6dOno0+fPujWrZtFBkCkRqwlIgWZ83olJF57ByDS0tKEEEKcOnVK9OnTR3h5eQmtVis6dOggXnnlFbNen2/MewxEtqKhv7/NUUvm5ENkixr6+2v2S4v1CQwMRG5urlmNlMgesZaIlMN7LRIRkaqxkRERkaqxkRERkaqxkRERkaqxkRERkaqxkRERkaqxkRERkaqxkRERkao1+mtcLOXWB0X59ROkRrd+b+/0gefmwnoiNWtoPdlcI6uoqABw884GRGpVUVEBnU5n7TRYT9Qi3KmeNMJWnjr+f7W1tSguLoa7uzsqKioQGBiIoqIieHh4WDu1ZlVeXs6xq3DsQghUVFTA398fDg7Wf+We9XSTmn+nmkrNY29oPdncFZmDg4Pxe5k0Gg0AwMPDQ3X/AUrh2NU3dlu4EruF9WSKY1ff2BtST9Z/ykhERNQEbGRERKRqNt3ItFot5syZY5ffeMux2+fYLcmef64ce8seu81N9iAiIjKHTV+RERER3QkbGRERqRobGRERqRobGRERqZpNN7KUlBTcfffdcHZ2RmRkJH744Qdrp6S47du3Y/DgwfD394dGo8HGjRtN1gshMHv2bLRr1w4uLi6IjY3FkSNHrJOswpKTk9GzZ0+4u7vD19cXQ4cORUFBgck2VVVVSExMhLe3N9zc3DBs2DCUlpZaKWP1sodaAuy3nuy9lmy2kf3nP//BjBkzMGfOHOzduxfdu3dHXFwczp07Z+3UFFVZWYnu3bsjJSVFcv3ChQvx/vvvIzU1Fbt27YKrqyvi4uJQVVXVzJkqLzc3F4mJicjLy8OWLVtw/fp1DBw4EJWVlcZtpk+fji+++AKZmZnIzc1FcXExEhISrJi1+thLLQH2W092X0vCRvXq1UskJiYaH9fU1Ah/f3+RnJxsxawsC4DYsGGD8XFtba3Q6/Vi0aJFxlhZWZnQarXi3//+txUytKxz584JACI3N1cIcXOsrVq1EpmZmcZtfv75ZwFA7Ny501ppqo491pIQ9l1P9lZLNnlFdu3aNezZswexsbHGmIODA2JjY7Fz504rZta8CgsLUVJSYvJz0Ol0iIyMbJE/B4PBAADw8vICAOzZswfXr183GX9YWBiCgoJa5PgtgbX0O3uqJ3urJZtsZBcuXEBNTQ38/PxM4n5+figpKbFSVs3v1ljt4edQW1uLadOmoXfv3ggPDwdwc/xOTk7w9PQ02bYljt9SWEu/s5d6ssdasrm735N9SkxMxKFDh/Dtt99aOxUiVbPHWrLJK7K2bdvC0dGxzoya0tJS6PV6K2XV/G6NtaX/HCZPnowvv/wS2dnZxq8cAW6O/9q1aygrKzPZvqWN35JYS7+zh3qy11qyyUbm5OSEiIgIZGVlGWO1tbXIyspCVFSUFTNrXiEhIdDr9SY/h/LycuzatatF/ByEEJg8eTI2bNiAbdu2ISQkxGR9REQEWrVqZTL+goICnDp1qkWMvzmwln7XkuvJ7mvJ2rNN5GRkZAitVivS09PF4cOHxfjx44Wnp6coKSmxdmqKqqioEPn5+SI/P18AEP/4xz9Efn6+OHnypBBCiLffflt4enqKzz//XBw4cEAMGTJEhISEiKtXr1o586abNGmS0Ol0IicnR5w9e9a4XLlyxbjNxIkTRVBQkNi2bZvYvXu3iIqKElFRUVbMWn3spZaEsN96svdastlGJoQQH3zwgQgKChJOTk6iV69eIi8vz9opKS47O1sAqLOMHDlSCHFzyvCbb74p/Pz8hFarFf379xcFBQXWTVohUuMGINLS0ozbXL16Vbz00kuiTZs2onXr1uLxxx8XZ8+etV7SKmUPtSSE/daTvdcSv8aFiIhUzSbfIyMiImooNjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlI1NjIiIlK1/wcjfbv8Eqr5vwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 500x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(4, 0.70803684), (8, 0.6951017), (4, 0.3356046), (8, 0.8453005), (8, 0.849671), (8, 0.7732455)]\n"
          ]
        }
      ],
      "source": [
        "left_vf_digit = (6, 0.5)\n",
        "\n",
        "left_idx, left_attention = left_vf_digit\n",
        "input_data_left = np.array([x_test[left_idx]]) * left_attention\n",
        "\n",
        "right_vf_digit = (61, 1)\n",
        "\n",
        "right_idx, right_attention = right_vf_digit\n",
        "input_data_right = np.array([x_test[right_idx]]) * right_attention\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(5, 5))\n",
        "\n",
        "axs[0].imshow(input_data_left[0], cmap=\"binary\", vmax=1)\n",
        "axs[0].set_title('Left Image')\n",
        "\n",
        "axs[1].imshow(input_data_right[0], cmap=\"binary\", vmax=1)\n",
        "axs[1].set_title('Right Image')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "def compute_digit_model_predicts(model, *data):\n",
        "    '''\n",
        "        Returns the digit the model predicts for the given input data\n",
        "\n",
        "        Input:\n",
        "        model: keras.Model: model to be used for prediction\n",
        "        *data: np.array: input data to be used for prediction, could be one or two arrays \n",
        "    \n",
        "    '''\n",
        "    activations = None\n",
        "    if len(data) == 2:\n",
        "        activations = model([data[0], data[1]])[0]\n",
        "    else:\n",
        "        activations = model(data[0])[0]\n",
        "    \n",
        "    return (np.argmax(activations), np.max(activations))\n",
        "\n",
        "def compute_digit_output_models_predict(output_models, input_data_left, input_data_right):\n",
        "    output_digits = []\n",
        "    for out_model in output_models:\n",
        "        if is_double_visual_field_model(out_model):\n",
        "            output_digits.append(compute_digit_model_predicts(out_model, input_data_left, input_data_right))\n",
        "        else:\n",
        "            if is_left_visual_field_model(out_model):\n",
        "                output_digits.append(compute_digit_model_predicts(out_model, input_data_left))\n",
        "            else:\n",
        "                output_digits.append(compute_digit_model_predicts(out_model, input_data_right))\n",
        "    \n",
        "    return output_digits\n",
        "\n",
        "output_digits = compute_digit_output_models_predict(output_models, input_data_left, input_data_right)\n",
        "print(output_digits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_t1.save('models/model_t1/model_t1_50_128.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, out_modeel in enumerate(output_models):\n",
        "    out_model.save(f'models/model_t1_50_256/out_models/out_model_{i}.keras')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
